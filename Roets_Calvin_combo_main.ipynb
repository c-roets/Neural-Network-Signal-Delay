{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1HQhnNfZ79jT"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import natsort\n",
        "import re \n",
        "import sys\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Eerst delay_dataset uploaden naar google colab en wachten tot upload compleet is\n",
        "standaarddirectory = os.getcwd()                  #standaarddirectory van google colab opslaan zodat we altijd terug kunnen keren naar deze\n",
        "file_name = \"delay_dataset.zip\"                   #de naam van de zipfile met alle data\n",
        "if not os.path.exists(\"stud_train\"):              #Als de data nog niet uitgepakt is en deze folder dus nog niet bestaat, gaan we dit doen\n",
        "  try:\n",
        "    with ZipFile(file_name,'r') as zipper:\n",
        "      zipper.extractall()\n",
        "      print(\"extractie data klaar\")\n",
        "  except:\n",
        "    sys.exit(\"Fout bij extractie data, zeker dat deze goed is geupload?\")\n",
        "\n",
        "if not os.path.exists(\"stud_train_combined_ASDR\"):    #Een folder waarin we de gecombineerde ASDR train signalen van 1024 samples zullen opslaan\n",
        "  os.makedirs(\"stud_train_combined_ASDR\")\n",
        "if not os.path.exists(\"stud_train_combined_GAUSS\"):   #Een folder waarin we de gecombineerde GAUSS train signalen van 1024 samples zullen opslaan\n",
        "  os.makedirs(\"stud_train_combined_GAUSS\")\n",
        "if not os.path.exists(\"stud_train_combined_SBOX\"):    #Een folder waarin we de gecombineerde SBOX train signalen van 1024 samples zullen opslaan\n",
        "  os.makedirs(\"stud_train_combined_SBOX\")\n",
        "\n",
        "if not os.path.exists(\"stud_test_combined_ASDR\"):    #Een folder waarin we de gecombineerde ASDR test signalen van 1024 samples zullen opslaan\n",
        "  os.makedirs(\"stud_test_combined_ASDR\")\n",
        "if not os.path.exists(\"stud_test_combined_GAUSS\"):   #Een folder waarin we de gecombineerde GAUSS test signalen van 1024 samples zullen opslaan\n",
        "  os.makedirs(\"stud_test_combined_GAUSS\")\n",
        "if not os.path.exists(\"stud_test_combined_SBOX\"):    #Een folder waarin we de gecombineerde SBOX test signalen van 1024 samples zullen opslaan\n",
        "  os.makedirs(\"stud_test_combined_SBOX\")"
      ],
      "metadata": {
        "id": "mgGGxSjv8Icw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variable declaration\n",
        "train_set = 'stud_train'  #een string voor de folder met de trainsamples\n",
        "test_set = 'stud_test'    #een string voor de folder met de testsamples\n",
        "delaypiekASDR = []    #arrays met werkelijke delays tussen de pieken (cijfers voor training en validatie)\n",
        "delaypiekGAUSS = []\n",
        "delaypiekSBOX = []\n",
        "delaypiekASDRtest = []    #arrays met werkelijke delays tussen de pieken (cijfers voor testing)\n",
        "delaypiekGAUSStest = []\n",
        "delaypiekSBOXtest = []\n",
        "grafieknummer = 0 #variabele voor grafieknummering"
      ],
      "metadata": {
        "id": "sokPDAPy8KV4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Geeft een array met strings weer waarin de namen van de files in die map staan\n",
        "def laadarraymetfilenamenuitmap(map):\n",
        "    filenames = os.listdir(map)  # array met alle filenames van in folder\n",
        "    filenames.sort()\n",
        "    return filenames"
      ],
      "metadata": {
        "id": "TgGQgK3V8M1u"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot het signaal van een numpy array aan de hand van een megegeven signaalbestandsnaam\n",
        "#Functie werd enkel gebruikt in het begin voor de twee verschillende signalen van 512 samples te visualiseren op 1 grafiek, niet meer nodig en niet meer gebruikt verder\n",
        "def plotsignaal(signaal, evencheck, nummer, x = np.arange(0,512)): #signaal is naam bestand tussen aanhalingstekens\n",
        "    if evencheck % 2 == 0:                                        #evencheck wordt gebruikt om elke keer twee signalen samen op 1 grafiek te zetten met elk een verschillend kleur\n",
        "        kleur = \"red\"         #paar van signalen respectievelijk rood en blauw weergeven zodat het verschil duidelijk is\n",
        "    else:\n",
        "        kleur = \"blue\"\n",
        "    plot = plt.figure(nummer)\n",
        "    plt.xlabel(\"samples\")\n",
        "    plt.ylabel(\"amplitude\")\n",
        "    plt.plot(x,np.load(signaal), color=kleur)"
      ],
      "metadata": {
        "id": "ddKNHGvW8Ok1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Geeft de waarde van de positie van die piek terug aan de hand van de bestandsnaam\n",
        "def geefpositiepiek(signaal):\n",
        "  truncatedsignaal = (signaal.split(\"P_\",1)[1]) #alles voor de xxx wegdoen dus BV ASDR0_P_ wegdoen\n",
        "  piek = ''.join(x for x in truncatedsignaal[0:3] if x.isdigit()) #drie eerstvolgende karakters overlopen en als deze een cijfer is, maakt deze deel uit van de piek bv: 211_SNR20 wordt 211, 8_SNR26 wordt 8\n",
        "  return piek"
      ],
      "metadata": {
        "id": "W3pSnBvJ8QUP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#voegt de array van twee signalen samen om 1 vector van 1024 samples te bekomen\n",
        "def voegsamen(signaal1, signaal2):\n",
        "  return np.concatenate([np.load(signaal1), np.load(signaal2)])"
      ],
      "metadata": {
        "id": "xzRdHkg08R93"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordt gebruikt om de lijst met de files te sorten op numerische orde ipv alfabetische orde om sortering van 0 1 10 11 ... 19 2 20 21 ... 29 3 30 31 etc te voorkomen maar 0 1 2 3 4 ... 99\n",
        "#outsourced functies\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "\n",
        "def natural_keys(text):\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
      ],
      "metadata": {
        "id": "RIRBSs8k8TcZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordt gebruikt om de 200 signalen van 512 te combineren naar 100 van 1024\n",
        "def combinedata(folder, dir, delayarray, typesignaal = \"\", testoftrain = \"\", aantalgrafiekenplotten = 0):\n",
        "  nummer, evencheck = 1, 0\n",
        "  for data in folder:\n",
        "    evencheck += 1\n",
        "    if evencheck % 2 == 0:     #controleert als we aan een even file zitten, zodat we deze kunnen samenvoegen met de oneven file die ervoor kwam\n",
        "      os.chdir(f\"stud_{testoftrain}\")         #folder binnengaan waar we willen samenvoegen\n",
        "      masterfile = voegsamen(previous, data) #masterfile is een array van 1024 nu\n",
        "      os.chdir(standaarddirectory)\n",
        "      os.chdir(dir)                         #folder binnengaan waar we willen saven\n",
        "      savename = f\"{nummer}{typesignaal}.npy\"\n",
        "      np.save(savename,masterfile)\n",
        "      os.chdir(standaarddirectory)\n",
        "      nummer += 1\n",
        "      delayarray.append(512-int(geefpositiepiek(previous))+int(geefpositiepiek(data)))    #512-piek1+piek2 is waarde van delay\n",
        "\n",
        "      #plotten voor eerste paar grafieken, mooie visualisatie als controle, wordt als default niet gedaan want variabele aantalgrafiekenplotten is 0\n",
        "      if nummer <= aantalgrafiekenplotten:  #max 20 grafieken door memory limiet, dus aantalgrafiekenplotten niet groter dan 20 maken\n",
        "        plot= plt.figure(nummer)\n",
        "        plt.plot(np.arange(0, 1024),masterfile, color=\"red\")\n",
        "\n",
        "    previous = data"
      ],
      "metadata": {
        "id": "COoxeWEx8VB9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotpredictieerrors(predictionslist, delaypiek, titel = '', typesign = 'test'):\n",
        "  global grafieknummer\n",
        "\n",
        "  #x zorgt voor een juiste dimensionering van de x as in de grafiek\n",
        "  if typesign == 'test':\n",
        "    x = 100+1\n",
        "  elif typesign == 'train':\n",
        "    x = 90+1\n",
        "  elif typesign == 'validation':\n",
        "    x = 10+1\n",
        "  elif typesign == 'totaltest':\n",
        "    x = 3*100+1\n",
        "  elif typesign == 'totaltrain':\n",
        "    x = 3*90+1\n",
        "  elif typesign == 'totalvalidation':\n",
        "    x = 3*10+1\n",
        "\n",
        "  error_list = []   #absolute waarde van de fout van predictie \n",
        "  for error1, error2 in zip(predictionslist, delaypiek):\n",
        "    error_list.append(round(abs(error1 - error2)))\n",
        "\n",
        "  y = (sum(error_list)/len(error_list)) #y waarde voor schaling y as grafiek\n",
        "  print(\"De errors zijn (afgerond tot gehele waarden):\")\n",
        "  print(error_list)\n",
        "  print(\"De predicties zijn:\")\n",
        "  print(predictionslist)\n",
        "  print(\"De werkelijke waarden zijn:\")\n",
        "  print(delaypiek)\n",
        "\n",
        "  #visualiseren error\n",
        "  bar = plt.figure(grafieknummer)\n",
        "  grafieknummer += 1\n",
        "  plt.bar(np.arange(1, x), error_list)\n",
        "  plt.ylim(0, y+25)\n",
        "  plt.title(titel)\n",
        "  plt.show()\n",
        "  print() "
      ],
      "metadata": {
        "id": "fTOu4cxI8W0p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "#filenames inlezen\n",
        "filenames = laadarraymetfilenamenuitmap(train_set)\n",
        "filenamesASDR = filenames[0:200]\n",
        "filenamesGAUSS = filenames[200:400]\n",
        "filenamesSBOX = filenames[400:600]\n",
        "\n",
        "filenamestest = laadarraymetfilenamenuitmap(test_set)\n",
        "filenamesASDRtest = filenamestest[0:200]\n",
        "filenamesGAUSStest = filenamestest[200:400]\n",
        "filenamesSBOXtest = filenamestest[400:600]\n",
        "\n",
        "#sorteren op numerische wijze ipv alfabetische wijze\n",
        "filenamesASDR.sort(key=natural_keys)\n",
        "filenamesGAUSS.sort(key=natural_keys)\n",
        "filenamesSBOX.sort(key=natural_keys)\n",
        "filenamesASDRtest.sort(key=natural_keys)\n",
        "filenamesGAUSStest.sort(key=natural_keys)\n",
        "filenamesSBOXtest.sort(key=natural_keys)"
      ],
      "metadata": {
        "id": "UDMNtMCl8ZeF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(standaarddirectory)\n",
        "#Nu gaan we de 6 folders opvullen waarin de files gecombineerd worden tot 100 signalen van 1024 samples\n",
        "combinedata(filenamesASDR, \"stud_train_combined_ASDR\", delaypiekASDR, \"ASDR\", \"train\")\n",
        "combinedata(filenamesGAUSS, \"stud_train_combined_GAUSS\", delaypiekGAUSS, \"GAUSS\", \"train\")\n",
        "combinedata(filenamesSBOX, \"stud_train_combined_SBOX\", delaypiekSBOX, \"SBOX\", \"train\")\n",
        "\n",
        "combinedata(filenamesASDRtest, \"stud_test_combined_ASDR\", delaypiekASDRtest, \"ASDR\", \"test\")\n",
        "combinedata(filenamesGAUSStest, \"stud_test_combined_GAUSS\", delaypiekGAUSStest, \"GAUSS\", \"test\")\n",
        "combinedata(filenamesSBOXtest, \"stud_test_combined_SBOX\", delaypiekSBOXtest, \"SBOX\", \"test\")"
      ],
      "metadata": {
        "id": "T7z9o7Qt8j2r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TENSOR VAN INDIVIDUELE MODELLEN AANMAKEN (ZODAT ZE KUNNEN SAMENGEVOEGD WORDEN LATER)"
      ],
      "metadata": {
        "id": "xSd4YnRa8nTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR Object maken voor x waarden NN training\n",
        "os.chdir(standaarddirectory)\n",
        "os.chdir(\"stud_train_combined_GAUSS\")\n",
        "\n",
        "#Traindata\n",
        "#We gebruiken deze traindata om het NN te trainen\n",
        "samplesGAUSS = [np.load(f\"{x}GAUSS.npy\") for x in range(1, 91)]\n",
        "samplesGAUSS = tf.convert_to_tensor(samplesGAUSS)\n",
        "\n",
        "#Validatiedata\n",
        "#We gebruiken de laatste 10% als validatiedata, deze data gebruiken we om de performantie van het NN te evalueren TIJDENS het trainen en zo de hyperparameters aan te passen. Het nn ziet validatiedata NIET op voorhand, in tegenstelling tot trainingsdata\n",
        "samplesGAUSSvalidation = [np.load(f\"{x}GAUSS.npy\") for x in range(91, 101)]\n",
        "samplesGAUSSvalidation = tf.convert_to_tensor(samplesGAUSSvalidation)\n",
        "\n",
        "os.chdir(standaarddirectory)"
      ],
      "metadata": {
        "id": "ZXsZeOvU_WJX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR Object maken voor x waarden NN testing\n",
        "os.chdir(standaarddirectory)\n",
        "os.chdir(\"stud_test_combined_GAUSS\")\n",
        "\n",
        "samplesGAUSStest = [np.load(f\"{x}GAUSS.npy\") for x in range(1, 101)]\n",
        "samplesGAUSStest = tf.convert_to_tensor(samplesGAUSStest)\n",
        "\n",
        "os.chdir(standaarddirectory)"
      ],
      "metadata": {
        "id": "2I_FcVpHBSJa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR object maken voor y waarden NN training (de delaywaarden tussen de pieken)\n",
        "\n",
        "#Traindata\n",
        "delayGAUSS = pd.DataFrame(data = delaypiekGAUSS[0:90])\n",
        "delayGAUSS = tf.convert_to_tensor(delayGAUSS)\n",
        "\n",
        "#Validatiedata\n",
        "delayGAUSSvalidation = pd.DataFrame(data = delaypiekGAUSS[90:])\n",
        "delayGAUSSvalidation = tf.convert_to_tensor(delayGAUSSvalidation)"
      ],
      "metadata": {
        "id": "5rpt0lGH_vUa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR Object maken voor x waarden NN training\n",
        "os.chdir(standaarddirectory)\n",
        "os.chdir(\"stud_train_combined_ASDR\")\n",
        "\n",
        "#Traindata\n",
        "#We gebruiken deze traindata om het NN te trainen\n",
        "samplesASDR = [np.load(f\"{x}ASDR.npy\") for x in range(1, 91)]\n",
        "samplesASDR = tf.convert_to_tensor(samplesASDR)\n",
        "\n",
        "#Validatiedata\n",
        "#We gebruiken de laatste 10% als validatiedata, deze data gebruiken we om de performantie van het NN te evalueren TIJDENS het trainen en zo de hyperparameters aan te passen. Het nn ziet validatiedata NIET op voorhand, in tegenstelling tot trainingsdata\n",
        "samplesASDRvalidation = [np.load(f\"{x}ASDR.npy\") for x in range(91, 101)]\n",
        "samplesASDRvalidation = tf.convert_to_tensor(samplesASDRvalidation)\n",
        "\n",
        "os.chdir(standaarddirectory)"
      ],
      "metadata": {
        "id": "7OHpqVbc_v3J"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR Object maken voor x waarden NN testing\n",
        "os.chdir(standaarddirectory)\n",
        "os.chdir(\"stud_test_combined_ASDR\")\n",
        "\n",
        "#Testdata\n",
        "#We gebruiken deze testdata om het NN te evalueren\n",
        "samplesASDRtest = [np.load(f\"{x}ASDR.npy\") for x in range(1, 101)]\n",
        "samplesASDRtest = tf.convert_to_tensor(samplesASDRtest)\n",
        "\n",
        "os.chdir(standaarddirectory)"
      ],
      "metadata": {
        "id": "udPxkF-fBT8i"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR object maken voor y waarden NN training (de delaywaarden tussen de pieken)\n",
        "\n",
        "#Traindata\n",
        "delayASDR = pd.DataFrame(data = delaypiekASDR[0:90])\n",
        "delayASDR = tf.convert_to_tensor(delayASDR)\n",
        "\n",
        "#Validatiedata\n",
        "delayASDRvalidation = pd.DataFrame(data = delaypiekASDR[90:])\n",
        "delayASDRvalidation = tf.convert_to_tensor(delayASDRvalidation)"
      ],
      "metadata": {
        "id": "MqdgSeCn_1EM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR Object maken voor x waarden NN training\n",
        "os.chdir(standaarddirectory)\n",
        "os.chdir(\"stud_train_combined_SBOX\")\n",
        "\n",
        "#Traindata\n",
        "#We gebruiken deze traindata om het NN te trainen\n",
        "samplesSBOX = [np.load(f\"{x}SBOX.npy\") for x in range(1, 91)]\n",
        "samplesSBOX = tf.convert_to_tensor(samplesSBOX)\n",
        "\n",
        "#Validatiedata\n",
        "#We gebruiken de laatste 10% als validatiedata, deze data gebruiken we om de performantie van het NN te evalueren TIJDENS het trainen en zo de hyperparameters aan te passen. Het nn ziet validatiedata NIET op voorhand, in tegenstelling tot trainingsdata\n",
        "samplesSBOXvalidation = [np.load(f\"{x}SBOX.npy\") for x in range(91, 101)]\n",
        "samplesSBOXvalidation = tf.convert_to_tensor(samplesSBOXvalidation)\n",
        "\n",
        "os.chdir(standaarddirectory)"
      ],
      "metadata": {
        "id": "H-kyztLe_3HV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR Object maken voor x waarden NN testing\n",
        "os.chdir(standaarddirectory)\n",
        "os.chdir(\"stud_test_combined_SBOX\")\n",
        "\n",
        "samplesSBOXtest = [np.load(f\"{x}SBOX.npy\") for x in range(1, 101)]\n",
        "samplesSBOXtest = tf.convert_to_tensor(samplesSBOXtest)\n",
        "\n",
        "os.chdir(standaarddirectory)\n"
      ],
      "metadata": {
        "id": "N6-LYS7uBcim"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR object maken voor y waarden NN training (de delaywaarden tussen de pieken)\n",
        "\n",
        "#Traindata\n",
        "delaySBOX = pd.DataFrame(data = delaypiekSBOX[0:90])\n",
        "delaySBOX = tf.convert_to_tensor(delaySBOX)\n",
        "\n",
        "#Validatiedata\n",
        "delaySBOXvalidation = pd.DataFrame(data = delaypiekSBOX[90:])\n",
        "delaySBOXvalidation = tf.convert_to_tensor(delaySBOXvalidation)"
      ],
      "metadata": {
        "id": "vQegVq2y_5lW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TENSOR VOOR TOTAAL MODEL"
      ],
      "metadata": {
        "id": "ut5AgRPV_fQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR VAN 300X1024\n",
        "samplestotal = tf.concat([samplesASDR, tf.concat([samplesGAUSS, samplesSBOX], axis=0)], axis=0)\n",
        "samplestotalvalidation = tf.concat([samplesASDRvalidation, tf.concat([samplesGAUSSvalidation, samplesSBOXvalidation], axis=0)], axis=0)\n",
        "samplestotaltest = tf.concat([samplesASDRtest, tf.concat([samplesGAUSStest, samplesSBOXtest], axis=0)], axis=0)\n",
        "\n",
        "delaytotal = tf.concat([delayASDR, tf.concat([delayGAUSS, delaySBOX], axis=0)], axis=0)\n",
        "delaytotalvalidation = tf.concat([delayASDRvalidation, tf.concat([delayGAUSSvalidation, delaySBOXvalidation], axis=0)], axis=0)"
      ],
      "metadata": {
        "id": "tWaRs95a8lPl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model aanmaken\n",
        "modeltotal = tf.keras.Sequential()\n",
        "modeltotal.add(layers.Dense(512, activation='linear', input_shape=(1024,)))\n",
        "modeltotal.add(layers.Dense(256, activation='linear'))\n",
        "modeltotal.add(layers.Dense(64, activation='linear'))\n",
        "modeltotal.add(layers.Dense(8, activation='linear'))\n",
        "modeltotal.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "L2afeqkL-N-Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeltotal.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu6q16vo-PgQ",
        "outputId": "95b3d621-5a28-4ae9-c0d0-ef80cef52c4c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 520       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 673,105\n",
            "Trainable params: 673,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer, loss function en metrics toevoegen aan model en compileren\n",
        "modeltotal.compile(optimizer='Adam', loss='mse', metrics=['mse'])\n",
        "K.set_value(modeltotal.optimizer.learning_rate, 0.0001)"
      ],
      "metadata": {
        "id": "seqfeZ-0-Q2G"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Trainen\n",
        "history4 = modeltotal.fit(samplestotal,\n",
        "                        delaytotal,\n",
        "                        epochs=600,\n",
        "                        validation_data=(samplestotalvalidation,delaytotalvalidation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG_ytB_Y-SHP",
        "outputId": "dd5cb7d0-3591-46d7-9de3-09f83403334c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 319911.8125 - mse: 319911.8125 - val_loss: 301253.8125 - val_mse: 301253.8125\n",
            "Epoch 2/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 283596.5312 - mse: 283596.5312 - val_loss: 262188.8125 - val_mse: 262188.8125\n",
            "Epoch 3/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 241976.3906 - mse: 241976.3906 - val_loss: 213074.9219 - val_mse: 213074.9219\n",
            "Epoch 4/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 189612.1562 - mse: 189612.1562 - val_loss: 160082.8594 - val_mse: 160082.8594\n",
            "Epoch 5/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 140139.6406 - mse: 140139.6406 - val_loss: 121259.0547 - val_mse: 121259.0547\n",
            "Epoch 6/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 115023.3828 - mse: 115023.3828 - val_loss: 116319.6016 - val_mse: 116319.6016\n",
            "Epoch 7/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 114400.5156 - mse: 114400.5156 - val_loss: 118359.9219 - val_mse: 118359.9219\n",
            "Epoch 8/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 111928.9297 - mse: 111928.9297 - val_loss: 115895.8047 - val_mse: 115895.8047\n",
            "Epoch 9/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 109696.6406 - mse: 109696.6406 - val_loss: 114665.6484 - val_mse: 114665.6484\n",
            "Epoch 10/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 109273.1562 - mse: 109273.1562 - val_loss: 114984.9297 - val_mse: 114984.9297\n",
            "Epoch 11/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 108662.8438 - mse: 108662.8438 - val_loss: 114908.7031 - val_mse: 114908.7031\n",
            "Epoch 12/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 107963.1875 - mse: 107963.1875 - val_loss: 115699.1172 - val_mse: 115699.1172\n",
            "Epoch 13/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 107376.9219 - mse: 107376.9219 - val_loss: 116003.3906 - val_mse: 116003.3906\n",
            "Epoch 14/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 107013.4531 - mse: 107013.4531 - val_loss: 116323.6016 - val_mse: 116323.6016\n",
            "Epoch 15/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 106539.7031 - mse: 106539.7031 - val_loss: 115920.3672 - val_mse: 115920.3672\n",
            "Epoch 16/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 106156.7578 - mse: 106156.7578 - val_loss: 115282.8984 - val_mse: 115282.8984\n",
            "Epoch 17/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 105847.1641 - mse: 105847.1641 - val_loss: 115310.4531 - val_mse: 115310.4531\n",
            "Epoch 18/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 105494.8125 - mse: 105494.8125 - val_loss: 115334.7812 - val_mse: 115334.7812\n",
            "Epoch 19/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 105075.0469 - mse: 105075.0469 - val_loss: 115258.7812 - val_mse: 115258.7812\n",
            "Epoch 20/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 104780.3047 - mse: 104780.3047 - val_loss: 116004.2734 - val_mse: 116004.2734\n",
            "Epoch 21/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 104699.3828 - mse: 104699.3828 - val_loss: 116675.6328 - val_mse: 116675.6328\n",
            "Epoch 22/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 104463.8906 - mse: 104463.8906 - val_loss: 119460.0312 - val_mse: 119460.0312\n",
            "Epoch 23/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 103917.6719 - mse: 103917.6719 - val_loss: 118301.4453 - val_mse: 118301.4453\n",
            "Epoch 24/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 103495.9453 - mse: 103495.9453 - val_loss: 117040.2812 - val_mse: 117040.2812\n",
            "Epoch 25/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 103126.2422 - mse: 103126.2422 - val_loss: 117647.6328 - val_mse: 117647.6328\n",
            "Epoch 26/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 103042.0156 - mse: 103042.0156 - val_loss: 117206.8750 - val_mse: 117206.8750\n",
            "Epoch 27/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 102888.7031 - mse: 102888.7031 - val_loss: 116798.2578 - val_mse: 116798.2578\n",
            "Epoch 28/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 102405.7891 - mse: 102405.7891 - val_loss: 118262.2500 - val_mse: 118262.2500\n",
            "Epoch 29/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 101932.5234 - mse: 101932.5234 - val_loss: 117786.2266 - val_mse: 117786.2266\n",
            "Epoch 30/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 101683.1250 - mse: 101683.1250 - val_loss: 119030.2891 - val_mse: 119030.2891\n",
            "Epoch 31/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 101451.7266 - mse: 101451.7266 - val_loss: 119585.9297 - val_mse: 119585.9297\n",
            "Epoch 32/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 101085.0312 - mse: 101085.0312 - val_loss: 119729.7422 - val_mse: 119729.7422\n",
            "Epoch 33/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 100948.0859 - mse: 100948.0859 - val_loss: 119823.4453 - val_mse: 119823.4453\n",
            "Epoch 34/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 100453.7344 - mse: 100453.7344 - val_loss: 120530.0000 - val_mse: 120530.0000\n",
            "Epoch 35/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 100630.7422 - mse: 100630.7422 - val_loss: 121260.0547 - val_mse: 121260.0547\n",
            "Epoch 36/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 99898.4922 - mse: 99898.4922 - val_loss: 121507.0156 - val_mse: 121507.0156\n",
            "Epoch 37/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 99581.3125 - mse: 99581.3125 - val_loss: 123370.8047 - val_mse: 123370.8047\n",
            "Epoch 38/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 99137.3594 - mse: 99137.3594 - val_loss: 124441.3984 - val_mse: 124441.3984\n",
            "Epoch 39/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 99211.1016 - mse: 99211.1016 - val_loss: 124716.5000 - val_mse: 124716.5000\n",
            "Epoch 40/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 98316.6406 - mse: 98316.6406 - val_loss: 128236.3047 - val_mse: 128236.3047\n",
            "Epoch 41/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 98102.9531 - mse: 98102.9531 - val_loss: 126733.9688 - val_mse: 126733.9688\n",
            "Epoch 42/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 97945.1875 - mse: 97945.1875 - val_loss: 124688.8281 - val_mse: 124688.8281\n",
            "Epoch 43/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 97477.1250 - mse: 97477.1250 - val_loss: 127689.5703 - val_mse: 127689.5703\n",
            "Epoch 44/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 97138.6094 - mse: 97138.6094 - val_loss: 128015.6016 - val_mse: 128015.6016\n",
            "Epoch 45/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 96733.6953 - mse: 96733.6953 - val_loss: 129448.8047 - val_mse: 129448.8047\n",
            "Epoch 46/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 96260.0469 - mse: 96260.0469 - val_loss: 128825.1953 - val_mse: 128825.1953\n",
            "Epoch 47/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 95949.6016 - mse: 95949.6016 - val_loss: 129394.2891 - val_mse: 129394.2891\n",
            "Epoch 48/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 95588.4219 - mse: 95588.4219 - val_loss: 131202.2656 - val_mse: 131202.2656\n",
            "Epoch 49/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 95561.2578 - mse: 95561.2578 - val_loss: 134976.3438 - val_mse: 134976.3438\n",
            "Epoch 50/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 95084.6641 - mse: 95084.6641 - val_loss: 135224.9688 - val_mse: 135224.9688\n",
            "Epoch 51/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 94752.4297 - mse: 94752.4297 - val_loss: 138605.6719 - val_mse: 138605.6719\n",
            "Epoch 52/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 94258.0391 - mse: 94258.0391 - val_loss: 139025.3750 - val_mse: 139025.3750\n",
            "Epoch 53/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 93922.9297 - mse: 93922.9297 - val_loss: 141313.3438 - val_mse: 141313.3438\n",
            "Epoch 54/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 93764.7500 - mse: 93764.7500 - val_loss: 147727.2500 - val_mse: 147727.2500\n",
            "Epoch 55/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 93346.1953 - mse: 93346.1953 - val_loss: 148104.0938 - val_mse: 148104.0938\n",
            "Epoch 56/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 92534.8203 - mse: 92534.8203 - val_loss: 150923.6094 - val_mse: 150923.6094\n",
            "Epoch 57/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 92307.2500 - mse: 92307.2500 - val_loss: 152161.5000 - val_mse: 152161.5000\n",
            "Epoch 58/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 91957.2266 - mse: 91957.2266 - val_loss: 154485.5625 - val_mse: 154485.5625\n",
            "Epoch 59/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 91509.2891 - mse: 91509.2891 - val_loss: 153043.9219 - val_mse: 153043.9219\n",
            "Epoch 60/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 91257.5234 - mse: 91257.5234 - val_loss: 160754.6719 - val_mse: 160754.6719\n",
            "Epoch 61/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 90314.4922 - mse: 90314.4922 - val_loss: 165894.4688 - val_mse: 165894.4688\n",
            "Epoch 62/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 89992.2109 - mse: 89992.2109 - val_loss: 166038.8594 - val_mse: 166038.8594\n",
            "Epoch 63/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 89676.0156 - mse: 89676.0156 - val_loss: 174640.2188 - val_mse: 174640.2188\n",
            "Epoch 64/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 89416.8203 - mse: 89416.8203 - val_loss: 174703.3281 - val_mse: 174703.3281\n",
            "Epoch 65/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 88543.1875 - mse: 88543.1875 - val_loss: 183449.2656 - val_mse: 183449.2656\n",
            "Epoch 66/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 88303.7109 - mse: 88303.7109 - val_loss: 196002.6719 - val_mse: 196002.6719\n",
            "Epoch 67/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 87353.9531 - mse: 87353.9531 - val_loss: 198002.2031 - val_mse: 198002.2031\n",
            "Epoch 68/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 86877.3359 - mse: 86877.3359 - val_loss: 195104.1562 - val_mse: 195104.1562\n",
            "Epoch 69/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 86676.4531 - mse: 86676.4531 - val_loss: 203337.0469 - val_mse: 203337.0469\n",
            "Epoch 70/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 85773.6094 - mse: 85773.6094 - val_loss: 217130.1094 - val_mse: 217130.1094\n",
            "Epoch 71/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 85940.5469 - mse: 85940.5469 - val_loss: 217509.7344 - val_mse: 217509.7344\n",
            "Epoch 72/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 84686.1094 - mse: 84686.1094 - val_loss: 230933.1719 - val_mse: 230933.1719\n",
            "Epoch 73/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 84655.9219 - mse: 84655.9219 - val_loss: 236675.6719 - val_mse: 236675.6719\n",
            "Epoch 74/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 83069.5625 - mse: 83069.5625 - val_loss: 244332.4062 - val_mse: 244332.4062\n",
            "Epoch 75/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 83787.3828 - mse: 83787.3828 - val_loss: 250096.2500 - val_mse: 250096.2500\n",
            "Epoch 76/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 82814.1406 - mse: 82814.1406 - val_loss: 259687.6406 - val_mse: 259687.6406\n",
            "Epoch 77/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 81678.0156 - mse: 81678.0156 - val_loss: 276219.7188 - val_mse: 276219.7188\n",
            "Epoch 78/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 80788.7344 - mse: 80788.7344 - val_loss: 282030.0312 - val_mse: 282030.0312\n",
            "Epoch 79/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 80308.7109 - mse: 80308.7109 - val_loss: 300134.0312 - val_mse: 300134.0312\n",
            "Epoch 80/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 80311.7578 - mse: 80311.7578 - val_loss: 286296.2188 - val_mse: 286296.2188\n",
            "Epoch 81/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 80252.2031 - mse: 80252.2031 - val_loss: 308646.2188 - val_mse: 308646.2188\n",
            "Epoch 82/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 78214.6797 - mse: 78214.6797 - val_loss: 324960.0312 - val_mse: 324960.0312\n",
            "Epoch 83/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 77824.6016 - mse: 77824.6016 - val_loss: 331022.8125 - val_mse: 331022.8125\n",
            "Epoch 84/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 76670.7109 - mse: 76670.7109 - val_loss: 346531.7188 - val_mse: 346531.7188\n",
            "Epoch 85/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 77493.6406 - mse: 77493.6406 - val_loss: 365699.6875 - val_mse: 365699.6875\n",
            "Epoch 86/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 76302.1641 - mse: 76302.1641 - val_loss: 357240.0938 - val_mse: 357240.0938\n",
            "Epoch 87/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 75164.8594 - mse: 75164.8594 - val_loss: 373579.3750 - val_mse: 373579.3750\n",
            "Epoch 88/600\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 73262.5156 - mse: 73262.5156 - val_loss: 394990.1562 - val_mse: 394990.1562\n",
            "Epoch 89/600\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 72507.6719 - mse: 72507.6719 - val_loss: 399519.5625 - val_mse: 399519.5625\n",
            "Epoch 90/600\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 72258.0703 - mse: 72258.0703 - val_loss: 421655.7812 - val_mse: 421655.7812\n",
            "Epoch 91/600\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 70242.1719 - mse: 70242.1719 - val_loss: 426831.0312 - val_mse: 426831.0312\n",
            "Epoch 92/600\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 69247.7812 - mse: 69247.7812 - val_loss: 456658.1875 - val_mse: 456658.1875\n",
            "Epoch 93/600\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 68468.4297 - mse: 68468.4297 - val_loss: 472689.1250 - val_mse: 472689.1250\n",
            "Epoch 94/600\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 68334.0859 - mse: 68334.0859 - val_loss: 482820.9375 - val_mse: 482820.9375\n",
            "Epoch 95/600\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 66787.8516 - mse: 66787.8516 - val_loss: 516333.0625 - val_mse: 516333.0625\n",
            "Epoch 96/600\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 65543.6562 - mse: 65543.6562 - val_loss: 513373.2188 - val_mse: 513373.2188\n",
            "Epoch 97/600\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 65658.9766 - mse: 65658.9766 - val_loss: 540205.5625 - val_mse: 540205.5625\n",
            "Epoch 98/600\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 67743.0469 - mse: 67743.0469 - val_loss: 513782.4688 - val_mse: 513782.4688\n",
            "Epoch 99/600\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 63434.5625 - mse: 63434.5625 - val_loss: 604550.8750 - val_mse: 604550.8750\n",
            "Epoch 100/600\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 63954.8594 - mse: 63954.8594 - val_loss: 571484.1875 - val_mse: 571484.1875\n",
            "Epoch 101/600\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 61585.9375 - mse: 61585.9375 - val_loss: 549481.3750 - val_mse: 549481.3750\n",
            "Epoch 102/600\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 60770.3828 - mse: 60770.3828 - val_loss: 619709.0000 - val_mse: 619709.0000\n",
            "Epoch 103/600\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 59480.8008 - mse: 59480.8008 - val_loss: 627275.9375 - val_mse: 627275.9375\n",
            "Epoch 104/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 56828.5117 - mse: 56828.5117 - val_loss: 624603.3750 - val_mse: 624603.3750\n",
            "Epoch 105/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 56456.1992 - mse: 56456.1992 - val_loss: 636474.6875 - val_mse: 636474.6875\n",
            "Epoch 106/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 53813.5273 - mse: 53813.5273 - val_loss: 663747.8125 - val_mse: 663747.8125\n",
            "Epoch 107/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 52622.4805 - mse: 52622.4805 - val_loss: 702502.6875 - val_mse: 702502.6875\n",
            "Epoch 108/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 51178.2969 - mse: 51178.2969 - val_loss: 714332.9375 - val_mse: 714332.9375\n",
            "Epoch 109/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 52391.1055 - mse: 52391.1055 - val_loss: 727454.5625 - val_mse: 727454.5625\n",
            "Epoch 110/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 51292.3203 - mse: 51292.3203 - val_loss: 731615.4375 - val_mse: 731615.4375\n",
            "Epoch 111/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 50372.4961 - mse: 50372.4961 - val_loss: 708659.9375 - val_mse: 708659.9375\n",
            "Epoch 112/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 48289.2500 - mse: 48289.2500 - val_loss: 729226.0000 - val_mse: 729226.0000\n",
            "Epoch 113/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 48284.6719 - mse: 48284.6719 - val_loss: 771877.8125 - val_mse: 771877.8125\n",
            "Epoch 114/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 47702.5352 - mse: 47702.5352 - val_loss: 724450.3125 - val_mse: 724450.3125\n",
            "Epoch 115/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 44270.8242 - mse: 44270.8242 - val_loss: 729120.6875 - val_mse: 729120.6875\n",
            "Epoch 116/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 42461.2148 - mse: 42461.2148 - val_loss: 788710.0000 - val_mse: 788710.0000\n",
            "Epoch 117/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 42216.8555 - mse: 42216.8555 - val_loss: 733616.3125 - val_mse: 733616.3125\n",
            "Epoch 118/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 38113.9961 - mse: 38113.9961 - val_loss: 718640.3750 - val_mse: 718640.3750\n",
            "Epoch 119/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 37805.7891 - mse: 37805.7891 - val_loss: 764040.3125 - val_mse: 764040.3125\n",
            "Epoch 120/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 39687.6641 - mse: 39687.6641 - val_loss: 815305.6250 - val_mse: 815305.6250\n",
            "Epoch 121/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 37718.3789 - mse: 37718.3789 - val_loss: 753946.0000 - val_mse: 753946.0000\n",
            "Epoch 122/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 34210.7305 - mse: 34210.7305 - val_loss: 727508.9375 - val_mse: 727508.9375\n",
            "Epoch 123/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 39680.4570 - mse: 39680.4570 - val_loss: 763503.3125 - val_mse: 763503.3125\n",
            "Epoch 124/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 36545.8750 - mse: 36545.8750 - val_loss: 737066.7500 - val_mse: 737066.7500\n",
            "Epoch 125/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 37410.1953 - mse: 37410.1953 - val_loss: 739680.5625 - val_mse: 739680.5625\n",
            "Epoch 126/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 35229.2852 - mse: 35229.2852 - val_loss: 723840.3750 - val_mse: 723840.3750\n",
            "Epoch 127/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 32959.8320 - mse: 32959.8320 - val_loss: 665715.2500 - val_mse: 665715.2500\n",
            "Epoch 128/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 30166.0391 - mse: 30166.0391 - val_loss: 740381.9375 - val_mse: 740381.9375\n",
            "Epoch 129/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 29161.7988 - mse: 29161.7988 - val_loss: 680912.6875 - val_mse: 680912.6875\n",
            "Epoch 130/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 29081.1348 - mse: 29081.1348 - val_loss: 706683.3125 - val_mse: 706683.3125\n",
            "Epoch 131/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 26000.5039 - mse: 26000.5039 - val_loss: 626831.7500 - val_mse: 626831.7500\n",
            "Epoch 132/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 26325.1250 - mse: 26325.1250 - val_loss: 697394.8125 - val_mse: 697394.8125\n",
            "Epoch 133/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 26194.5977 - mse: 26194.5977 - val_loss: 700054.6875 - val_mse: 700054.6875\n",
            "Epoch 134/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 24395.2070 - mse: 24395.2070 - val_loss: 678091.6250 - val_mse: 678091.6250\n",
            "Epoch 135/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 24556.6367 - mse: 24556.6367 - val_loss: 636659.4375 - val_mse: 636659.4375\n",
            "Epoch 136/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 22920.1836 - mse: 22920.1836 - val_loss: 654586.4375 - val_mse: 654586.4375\n",
            "Epoch 137/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 22485.5625 - mse: 22485.5625 - val_loss: 609362.2500 - val_mse: 609362.2500\n",
            "Epoch 138/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 20280.9551 - mse: 20280.9551 - val_loss: 614048.3125 - val_mse: 614048.3125\n",
            "Epoch 139/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 18933.3496 - mse: 18933.3496 - val_loss: 605448.4375 - val_mse: 605448.4375\n",
            "Epoch 140/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 19623.5859 - mse: 19623.5859 - val_loss: 608591.0000 - val_mse: 608591.0000\n",
            "Epoch 141/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 20451.4277 - mse: 20451.4277 - val_loss: 555572.9375 - val_mse: 555572.9375\n",
            "Epoch 142/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 20873.0938 - mse: 20873.0938 - val_loss: 548648.8750 - val_mse: 548648.8750\n",
            "Epoch 143/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 19405.7363 - mse: 19405.7363 - val_loss: 513705.9375 - val_mse: 513705.9375\n",
            "Epoch 144/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 24098.7852 - mse: 24098.7852 - val_loss: 508327.0625 - val_mse: 508327.0625\n",
            "Epoch 145/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 20111.0059 - mse: 20111.0059 - val_loss: 456951.5625 - val_mse: 456951.5625\n",
            "Epoch 146/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 21515.9648 - mse: 21515.9648 - val_loss: 459047.8125 - val_mse: 459047.8125\n",
            "Epoch 147/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 21917.7227 - mse: 21917.7227 - val_loss: 452676.4688 - val_mse: 452676.4688\n",
            "Epoch 148/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 21979.1504 - mse: 21979.1504 - val_loss: 434763.5625 - val_mse: 434763.5625\n",
            "Epoch 149/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 21768.9941 - mse: 21768.9941 - val_loss: 449669.8438 - val_mse: 449669.8438\n",
            "Epoch 150/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 19308.8418 - mse: 19308.8418 - val_loss: 450582.9062 - val_mse: 450582.9062\n",
            "Epoch 151/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 14815.8184 - mse: 14815.8184 - val_loss: 409552.2188 - val_mse: 409552.2188\n",
            "Epoch 152/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 13823.1514 - mse: 13823.1514 - val_loss: 403176.0312 - val_mse: 403176.0312\n",
            "Epoch 153/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 13357.2529 - mse: 13357.2529 - val_loss: 421693.0312 - val_mse: 421693.0312\n",
            "Epoch 154/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 12945.1699 - mse: 12945.1699 - val_loss: 388675.1562 - val_mse: 388675.1562\n",
            "Epoch 155/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 12546.5068 - mse: 12546.5068 - val_loss: 443139.1562 - val_mse: 443139.1562\n",
            "Epoch 156/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 12318.0244 - mse: 12318.0244 - val_loss: 374010.1250 - val_mse: 374010.1250\n",
            "Epoch 157/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 11887.4395 - mse: 11887.4395 - val_loss: 389061.2812 - val_mse: 389061.2812\n",
            "Epoch 158/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 11524.7412 - mse: 11524.7412 - val_loss: 361006.0625 - val_mse: 361006.0625\n",
            "Epoch 159/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 11081.1904 - mse: 11081.1904 - val_loss: 363478.9688 - val_mse: 363478.9688\n",
            "Epoch 160/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 10652.2266 - mse: 10652.2266 - val_loss: 328351.3125 - val_mse: 328351.3125\n",
            "Epoch 161/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 10629.5605 - mse: 10629.5605 - val_loss: 343194.9062 - val_mse: 343194.9062\n",
            "Epoch 162/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 10005.3486 - mse: 10005.3486 - val_loss: 332781.5625 - val_mse: 332781.5625\n",
            "Epoch 163/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 10067.9219 - mse: 10067.9219 - val_loss: 334341.9375 - val_mse: 334341.9375\n",
            "Epoch 164/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 10336.4053 - mse: 10336.4053 - val_loss: 315865.0312 - val_mse: 315865.0312\n",
            "Epoch 165/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 10221.6777 - mse: 10221.6777 - val_loss: 319738.3125 - val_mse: 319738.3125\n",
            "Epoch 166/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 9735.9912 - mse: 9735.9912 - val_loss: 283242.9375 - val_mse: 283242.9375\n",
            "Epoch 167/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 9070.2051 - mse: 9070.2051 - val_loss: 291062.6562 - val_mse: 291062.6562\n",
            "Epoch 168/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 8852.4521 - mse: 8852.4521 - val_loss: 293549.6562 - val_mse: 293549.6562\n",
            "Epoch 169/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 9257.3447 - mse: 9257.3447 - val_loss: 288058.0312 - val_mse: 288058.0312\n",
            "Epoch 170/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 9308.5283 - mse: 9308.5283 - val_loss: 264800.9688 - val_mse: 264800.9688\n",
            "Epoch 171/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 9183.6807 - mse: 9183.6807 - val_loss: 270191.3750 - val_mse: 270191.3750\n",
            "Epoch 172/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 8162.6602 - mse: 8162.6602 - val_loss: 262911.8438 - val_mse: 262911.8438\n",
            "Epoch 173/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 8740.0947 - mse: 8740.0947 - val_loss: 254571.8281 - val_mse: 254571.8281\n",
            "Epoch 174/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 8161.8574 - mse: 8161.8574 - val_loss: 252091.5156 - val_mse: 252091.5156\n",
            "Epoch 175/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 7983.1382 - mse: 7983.1382 - val_loss: 233821.5469 - val_mse: 233821.5469\n",
            "Epoch 176/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 8083.8999 - mse: 8083.8999 - val_loss: 245237.1875 - val_mse: 245237.1875\n",
            "Epoch 177/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 9537.6475 - mse: 9537.6475 - val_loss: 230422.7500 - val_mse: 230422.7500\n",
            "Epoch 178/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 19611.1113 - mse: 19611.1113 - val_loss: 175567.0938 - val_mse: 175567.0938\n",
            "Epoch 179/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 15505.6074 - mse: 15505.6074 - val_loss: 221827.7656 - val_mse: 221827.7656\n",
            "Epoch 180/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 27176.1035 - mse: 27176.1035 - val_loss: 225445.4375 - val_mse: 225445.4375\n",
            "Epoch 181/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 9475.5693 - mse: 9475.5693 - val_loss: 164845.0781 - val_mse: 164845.0781\n",
            "Epoch 182/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 10217.6445 - mse: 10217.6445 - val_loss: 168638.8125 - val_mse: 168638.8125\n",
            "Epoch 183/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 10404.8799 - mse: 10404.8799 - val_loss: 183997.9688 - val_mse: 183997.9688\n",
            "Epoch 184/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 11186.2637 - mse: 11186.2637 - val_loss: 197281.8281 - val_mse: 197281.8281\n",
            "Epoch 185/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 10351.0811 - mse: 10351.0811 - val_loss: 191693.8125 - val_mse: 191693.8125\n",
            "Epoch 186/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 10071.5811 - mse: 10071.5811 - val_loss: 171212.5625 - val_mse: 171212.5625\n",
            "Epoch 187/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 8236.7334 - mse: 8236.7334 - val_loss: 199785.0781 - val_mse: 199785.0781\n",
            "Epoch 188/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 7573.0054 - mse: 7573.0054 - val_loss: 196893.1562 - val_mse: 196893.1562\n",
            "Epoch 189/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 6827.4585 - mse: 6827.4585 - val_loss: 186988.6094 - val_mse: 186988.6094\n",
            "Epoch 190/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 7479.9268 - mse: 7479.9268 - val_loss: 198542.1719 - val_mse: 198542.1719\n",
            "Epoch 191/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6284.6558 - mse: 6284.6558 - val_loss: 187785.3125 - val_mse: 187785.3125\n",
            "Epoch 192/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5884.2505 - mse: 5884.2505 - val_loss: 167011.7656 - val_mse: 167011.7656\n",
            "Epoch 193/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5926.7378 - mse: 5926.7378 - val_loss: 179250.1719 - val_mse: 179250.1719\n",
            "Epoch 194/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6499.2061 - mse: 6499.2061 - val_loss: 183760.4219 - val_mse: 183760.4219\n",
            "Epoch 195/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 6866.7314 - mse: 6866.7314 - val_loss: 170534.6875 - val_mse: 170534.6875\n",
            "Epoch 196/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6826.1836 - mse: 6826.1836 - val_loss: 170818.9219 - val_mse: 170818.9219\n",
            "Epoch 197/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 7044.2148 - mse: 7044.2148 - val_loss: 168118.8594 - val_mse: 168118.8594\n",
            "Epoch 198/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 6957.4570 - mse: 6957.4570 - val_loss: 165053.9375 - val_mse: 165053.9375\n",
            "Epoch 199/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6795.7549 - mse: 6795.7549 - val_loss: 170436.3594 - val_mse: 170436.3594\n",
            "Epoch 200/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 6214.0205 - mse: 6214.0205 - val_loss: 169228.9688 - val_mse: 169228.9688\n",
            "Epoch 201/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 6324.7900 - mse: 6324.7900 - val_loss: 160702.1406 - val_mse: 160702.1406\n",
            "Epoch 202/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 7036.1582 - mse: 7036.1582 - val_loss: 157031.7188 - val_mse: 157031.7188\n",
            "Epoch 203/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5679.8022 - mse: 5679.8022 - val_loss: 157250.5312 - val_mse: 157250.5312\n",
            "Epoch 204/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 5777.5459 - mse: 5777.5459 - val_loss: 167588.2812 - val_mse: 167588.2812\n",
            "Epoch 205/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5554.6689 - mse: 5554.6689 - val_loss: 156429.3125 - val_mse: 156429.3125\n",
            "Epoch 206/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5000.9282 - mse: 5000.9282 - val_loss: 153815.0938 - val_mse: 153815.0938\n",
            "Epoch 207/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 5069.8550 - mse: 5069.8550 - val_loss: 159279.2344 - val_mse: 159279.2344\n",
            "Epoch 208/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 5383.4839 - mse: 5383.4839 - val_loss: 162620.7344 - val_mse: 162620.7344\n",
            "Epoch 209/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 5291.9062 - mse: 5291.9062 - val_loss: 150348.2344 - val_mse: 150348.2344\n",
            "Epoch 210/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5980.6299 - mse: 5980.6299 - val_loss: 161181.7812 - val_mse: 161181.7812\n",
            "Epoch 211/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5021.9966 - mse: 5021.9966 - val_loss: 148017.4688 - val_mse: 148017.4688\n",
            "Epoch 212/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4900.1597 - mse: 4900.1597 - val_loss: 136373.6875 - val_mse: 136373.6875\n",
            "Epoch 213/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4353.7051 - mse: 4353.7051 - val_loss: 136669.9375 - val_mse: 136669.9375\n",
            "Epoch 214/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4413.2158 - mse: 4413.2158 - val_loss: 133427.5156 - val_mse: 133427.5156\n",
            "Epoch 215/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4898.8350 - mse: 4898.8350 - val_loss: 132451.4844 - val_mse: 132451.4844\n",
            "Epoch 216/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5545.5205 - mse: 5545.5205 - val_loss: 131622.2344 - val_mse: 131622.2344\n",
            "Epoch 217/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5583.3467 - mse: 5583.3467 - val_loss: 132507.9375 - val_mse: 132507.9375\n",
            "Epoch 218/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4237.5132 - mse: 4237.5132 - val_loss: 140496.2969 - val_mse: 140496.2969\n",
            "Epoch 219/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5055.9307 - mse: 5055.9307 - val_loss: 132132.5469 - val_mse: 132132.5469\n",
            "Epoch 220/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 5296.3496 - mse: 5296.3496 - val_loss: 132284.3438 - val_mse: 132284.3438\n",
            "Epoch 221/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4400.9609 - mse: 4400.9609 - val_loss: 136167.8594 - val_mse: 136167.8594\n",
            "Epoch 222/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 4053.1523 - mse: 4053.1523 - val_loss: 135921.0312 - val_mse: 135921.0312\n",
            "Epoch 223/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4084.1602 - mse: 4084.1602 - val_loss: 124186.2891 - val_mse: 124186.2891\n",
            "Epoch 224/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3900.2278 - mse: 3900.2278 - val_loss: 123689.9688 - val_mse: 123689.9688\n",
            "Epoch 225/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3794.6709 - mse: 3794.6709 - val_loss: 128757.5000 - val_mse: 128757.5000\n",
            "Epoch 226/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3985.1714 - mse: 3985.1714 - val_loss: 135153.1406 - val_mse: 135153.1406\n",
            "Epoch 227/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 4325.1714 - mse: 4325.1714 - val_loss: 135296.6719 - val_mse: 135296.6719\n",
            "Epoch 228/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 8009.2456 - mse: 8009.2456 - val_loss: 114522.2031 - val_mse: 114522.2031\n",
            "Epoch 229/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4797.8931 - mse: 4797.8931 - val_loss: 100471.0547 - val_mse: 100471.0547\n",
            "Epoch 230/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 7671.8525 - mse: 7671.8525 - val_loss: 168683.8125 - val_mse: 168683.8125\n",
            "Epoch 231/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 10454.4502 - mse: 10454.4502 - val_loss: 141588.0469 - val_mse: 141588.0469\n",
            "Epoch 232/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 10048.3945 - mse: 10048.3945 - val_loss: 100672.9453 - val_mse: 100672.9453\n",
            "Epoch 233/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 7193.1050 - mse: 7193.1050 - val_loss: 124216.7422 - val_mse: 124216.7422\n",
            "Epoch 234/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6343.6782 - mse: 6343.6782 - val_loss: 103679.4766 - val_mse: 103679.4766\n",
            "Epoch 235/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 12463.5312 - mse: 12463.5312 - val_loss: 135428.3594 - val_mse: 135428.3594\n",
            "Epoch 236/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 6930.6514 - mse: 6930.6514 - val_loss: 114808.6484 - val_mse: 114808.6484\n",
            "Epoch 237/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 6742.9707 - mse: 6742.9707 - val_loss: 103333.9531 - val_mse: 103333.9531\n",
            "Epoch 238/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 10227.0488 - mse: 10227.0488 - val_loss: 121289.8984 - val_mse: 121289.8984\n",
            "Epoch 239/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 13528.1396 - mse: 13528.1396 - val_loss: 150909.2656 - val_mse: 150909.2656\n",
            "Epoch 240/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 9605.9863 - mse: 9605.9863 - val_loss: 153443.1094 - val_mse: 153443.1094\n",
            "Epoch 241/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 8801.6904 - mse: 8801.6904 - val_loss: 108411.0469 - val_mse: 108411.0469\n",
            "Epoch 242/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 8207.3945 - mse: 8207.3945 - val_loss: 81815.1172 - val_mse: 81815.1172\n",
            "Epoch 243/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6128.5342 - mse: 6128.5342 - val_loss: 134468.1094 - val_mse: 134468.1094\n",
            "Epoch 244/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 5069.2686 - mse: 5069.2686 - val_loss: 120727.9922 - val_mse: 120727.9922\n",
            "Epoch 245/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4550.7163 - mse: 4550.7163 - val_loss: 127557.2031 - val_mse: 127557.2031\n",
            "Epoch 246/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3663.1995 - mse: 3663.1995 - val_loss: 117542.7109 - val_mse: 117542.7109\n",
            "Epoch 247/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3792.9819 - mse: 3792.9819 - val_loss: 126902.7656 - val_mse: 126902.7656\n",
            "Epoch 248/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 7535.6284 - mse: 7535.6284 - val_loss: 109778.6406 - val_mse: 109778.6406\n",
            "Epoch 249/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5581.6421 - mse: 5581.6421 - val_loss: 122988.2188 - val_mse: 122988.2188\n",
            "Epoch 250/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 14299.4238 - mse: 14299.4238 - val_loss: 119848.0234 - val_mse: 119848.0234\n",
            "Epoch 251/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4940.1445 - mse: 4940.1445 - val_loss: 103327.7266 - val_mse: 103327.7266\n",
            "Epoch 252/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6344.7803 - mse: 6344.7803 - val_loss: 84127.1562 - val_mse: 84127.1562\n",
            "Epoch 253/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3992.4297 - mse: 3992.4297 - val_loss: 106819.5078 - val_mse: 106819.5078\n",
            "Epoch 254/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4010.3596 - mse: 4010.3596 - val_loss: 91301.6406 - val_mse: 91301.6406\n",
            "Epoch 255/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3605.0737 - mse: 3605.0737 - val_loss: 98013.6016 - val_mse: 98013.6016\n",
            "Epoch 256/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 3907.7427 - mse: 3907.7427 - val_loss: 107779.4531 - val_mse: 107779.4531\n",
            "Epoch 257/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3646.7944 - mse: 3646.7944 - val_loss: 102494.3672 - val_mse: 102494.3672\n",
            "Epoch 258/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 3191.0906 - mse: 3191.0906 - val_loss: 89946.5078 - val_mse: 89946.5078\n",
            "Epoch 259/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3491.5000 - mse: 3491.5000 - val_loss: 110354.3203 - val_mse: 110354.3203\n",
            "Epoch 260/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2859.0105 - mse: 2859.0105 - val_loss: 109151.6641 - val_mse: 109151.6641\n",
            "Epoch 261/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2662.9963 - mse: 2662.9963 - val_loss: 113310.6797 - val_mse: 113310.6797\n",
            "Epoch 262/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2468.6499 - mse: 2468.6499 - val_loss: 103018.6172 - val_mse: 103018.6172\n",
            "Epoch 263/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2385.5276 - mse: 2385.5276 - val_loss: 102938.7578 - val_mse: 102938.7578\n",
            "Epoch 264/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2337.8369 - mse: 2337.8369 - val_loss: 104290.4219 - val_mse: 104290.4219\n",
            "Epoch 265/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2229.6235 - mse: 2229.6235 - val_loss: 103395.1641 - val_mse: 103395.1641\n",
            "Epoch 266/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2151.6882 - mse: 2151.6882 - val_loss: 102142.3594 - val_mse: 102142.3594\n",
            "Epoch 267/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2193.3025 - mse: 2193.3025 - val_loss: 95925.3828 - val_mse: 95925.3828\n",
            "Epoch 268/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2168.6189 - mse: 2168.6189 - val_loss: 96898.9062 - val_mse: 96898.9062\n",
            "Epoch 269/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2048.7087 - mse: 2048.7087 - val_loss: 102193.9922 - val_mse: 102193.9922\n",
            "Epoch 270/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2394.3450 - mse: 2394.3450 - val_loss: 95021.2344 - val_mse: 95021.2344\n",
            "Epoch 271/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2299.0500 - mse: 2299.0500 - val_loss: 92360.8047 - val_mse: 92360.8047\n",
            "Epoch 272/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2267.6465 - mse: 2267.6465 - val_loss: 93857.3984 - val_mse: 93857.3984\n",
            "Epoch 273/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2370.9534 - mse: 2370.9534 - val_loss: 92608.6328 - val_mse: 92608.6328\n",
            "Epoch 274/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2028.3182 - mse: 2028.3182 - val_loss: 92177.1094 - val_mse: 92177.1094\n",
            "Epoch 275/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2122.6152 - mse: 2122.6152 - val_loss: 90827.4844 - val_mse: 90827.4844\n",
            "Epoch 276/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2177.3601 - mse: 2177.3601 - val_loss: 93500.3672 - val_mse: 93500.3672\n",
            "Epoch 277/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1918.7334 - mse: 1918.7334 - val_loss: 93720.5000 - val_mse: 93720.5000\n",
            "Epoch 278/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2014.8409 - mse: 2014.8409 - val_loss: 96866.5547 - val_mse: 96866.5547\n",
            "Epoch 279/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2021.9586 - mse: 2021.9586 - val_loss: 92137.4922 - val_mse: 92137.4922\n",
            "Epoch 280/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2090.6228 - mse: 2090.6228 - val_loss: 93150.8984 - val_mse: 93150.8984\n",
            "Epoch 281/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2044.7495 - mse: 2044.7495 - val_loss: 89208.4922 - val_mse: 89208.4922\n",
            "Epoch 282/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1984.5854 - mse: 1984.5854 - val_loss: 89364.9609 - val_mse: 89364.9609\n",
            "Epoch 283/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1941.7195 - mse: 1941.7195 - val_loss: 91634.7891 - val_mse: 91634.7891\n",
            "Epoch 284/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1902.1678 - mse: 1902.1678 - val_loss: 89258.6328 - val_mse: 89258.6328\n",
            "Epoch 285/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1847.3699 - mse: 1847.3699 - val_loss: 91384.5859 - val_mse: 91384.5859\n",
            "Epoch 286/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1815.9889 - mse: 1815.9889 - val_loss: 85761.4297 - val_mse: 85761.4297\n",
            "Epoch 287/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2012.1663 - mse: 2012.1663 - val_loss: 88766.8594 - val_mse: 88766.8594\n",
            "Epoch 288/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2002.9900 - mse: 2002.9900 - val_loss: 86436.8828 - val_mse: 86436.8828\n",
            "Epoch 289/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2185.8848 - mse: 2185.8848 - val_loss: 83330.4219 - val_mse: 83330.4219\n",
            "Epoch 290/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1874.8885 - mse: 1874.8885 - val_loss: 84819.8828 - val_mse: 84819.8828\n",
            "Epoch 291/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1983.2986 - mse: 1983.2986 - val_loss: 79661.6641 - val_mse: 79661.6641\n",
            "Epoch 292/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2013.3856 - mse: 2013.3856 - val_loss: 84893.4531 - val_mse: 84893.4531\n",
            "Epoch 293/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2587.9622 - mse: 2587.9622 - val_loss: 86199.0781 - val_mse: 86199.0781\n",
            "Epoch 294/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1992.1578 - mse: 1992.1578 - val_loss: 86027.6719 - val_mse: 86027.6719\n",
            "Epoch 295/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2227.9888 - mse: 2227.9888 - val_loss: 91546.7109 - val_mse: 91546.7109\n",
            "Epoch 296/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2516.2783 - mse: 2516.2783 - val_loss: 78912.1562 - val_mse: 78912.1562\n",
            "Epoch 297/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2574.8372 - mse: 2574.8372 - val_loss: 75394.3594 - val_mse: 75394.3594\n",
            "Epoch 298/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2376.8359 - mse: 2376.8359 - val_loss: 77322.2656 - val_mse: 77322.2656\n",
            "Epoch 299/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2063.6855 - mse: 2063.6855 - val_loss: 81750.7344 - val_mse: 81750.7344\n",
            "Epoch 300/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4151.9077 - mse: 4151.9077 - val_loss: 77473.8047 - val_mse: 77473.8047\n",
            "Epoch 301/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2418.3225 - mse: 2418.3225 - val_loss: 71647.7266 - val_mse: 71647.7266\n",
            "Epoch 302/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 2407.3516 - mse: 2407.3516 - val_loss: 80699.1094 - val_mse: 80699.1094\n",
            "Epoch 303/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 4027.2180 - mse: 4027.2180 - val_loss: 81771.1562 - val_mse: 81771.1562\n",
            "Epoch 304/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2193.4966 - mse: 2193.4966 - val_loss: 79672.2266 - val_mse: 79672.2266\n",
            "Epoch 305/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2345.5078 - mse: 2345.5078 - val_loss: 77351.6797 - val_mse: 77351.6797\n",
            "Epoch 306/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1886.2083 - mse: 1886.2083 - val_loss: 89518.7656 - val_mse: 89518.7656\n",
            "Epoch 307/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2093.5557 - mse: 2093.5557 - val_loss: 83142.6016 - val_mse: 83142.6016\n",
            "Epoch 308/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2971.5183 - mse: 2971.5183 - val_loss: 73662.6641 - val_mse: 73662.6641\n",
            "Epoch 309/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3958.2246 - mse: 3958.2246 - val_loss: 80718.8438 - val_mse: 80718.8438\n",
            "Epoch 310/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5260.9014 - mse: 5260.9014 - val_loss: 81129.3672 - val_mse: 81129.3672\n",
            "Epoch 311/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5478.0669 - mse: 5478.0669 - val_loss: 65505.2773 - val_mse: 65505.2773\n",
            "Epoch 312/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5550.6206 - mse: 5550.6206 - val_loss: 81016.2422 - val_mse: 81016.2422\n",
            "Epoch 313/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5503.2285 - mse: 5503.2285 - val_loss: 46935.8867 - val_mse: 46935.8867\n",
            "Epoch 314/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 5984.8813 - mse: 5984.8813 - val_loss: 54413.1992 - val_mse: 54413.1992\n",
            "Epoch 315/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4673.8276 - mse: 4673.8276 - val_loss: 67634.6484 - val_mse: 67634.6484\n",
            "Epoch 316/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2944.6577 - mse: 2944.6577 - val_loss: 77897.7188 - val_mse: 77897.7188\n",
            "Epoch 317/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2418.6636 - mse: 2418.6636 - val_loss: 76229.1094 - val_mse: 76229.1094\n",
            "Epoch 318/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4821.7705 - mse: 4821.7705 - val_loss: 77936.4531 - val_mse: 77936.4531\n",
            "Epoch 319/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3050.3845 - mse: 3050.3845 - val_loss: 80914.2266 - val_mse: 80914.2266\n",
            "Epoch 320/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3052.2407 - mse: 3052.2407 - val_loss: 76743.8984 - val_mse: 76743.8984\n",
            "Epoch 321/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2853.8311 - mse: 2853.8311 - val_loss: 87758.8438 - val_mse: 87758.8438\n",
            "Epoch 322/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2443.4136 - mse: 2443.4136 - val_loss: 93364.9688 - val_mse: 93364.9688\n",
            "Epoch 323/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3390.1892 - mse: 3390.1892 - val_loss: 82908.5781 - val_mse: 82908.5781\n",
            "Epoch 324/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3055.3240 - mse: 3055.3240 - val_loss: 85034.6328 - val_mse: 85034.6328\n",
            "Epoch 325/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 6664.4575 - mse: 6664.4575 - val_loss: 90702.9531 - val_mse: 90702.9531\n",
            "Epoch 326/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3923.0371 - mse: 3923.0371 - val_loss: 58269.2383 - val_mse: 58269.2383\n",
            "Epoch 327/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5171.5952 - mse: 5171.5952 - val_loss: 82920.8359 - val_mse: 82920.8359\n",
            "Epoch 328/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4522.8813 - mse: 4522.8813 - val_loss: 60418.9023 - val_mse: 60418.9023\n",
            "Epoch 329/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 5758.2148 - mse: 5758.2148 - val_loss: 73666.6016 - val_mse: 73666.6016\n",
            "Epoch 330/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5647.5269 - mse: 5647.5269 - val_loss: 65184.8633 - val_mse: 65184.8633\n",
            "Epoch 331/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5593.0229 - mse: 5593.0229 - val_loss: 82292.9531 - val_mse: 82292.9531\n",
            "Epoch 332/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4979.6704 - mse: 4979.6704 - val_loss: 76147.6641 - val_mse: 76147.6641\n",
            "Epoch 333/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 5948.7310 - mse: 5948.7310 - val_loss: 71468.4688 - val_mse: 71468.4688\n",
            "Epoch 334/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4098.5337 - mse: 4098.5337 - val_loss: 54902.9375 - val_mse: 54902.9375\n",
            "Epoch 335/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 6285.6094 - mse: 6285.6094 - val_loss: 78752.4609 - val_mse: 78752.4609\n",
            "Epoch 336/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5631.9805 - mse: 5631.9805 - val_loss: 63975.2148 - val_mse: 63975.2148\n",
            "Epoch 337/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5400.1743 - mse: 5400.1743 - val_loss: 83541.7266 - val_mse: 83541.7266\n",
            "Epoch 338/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 7046.5322 - mse: 7046.5322 - val_loss: 58121.5117 - val_mse: 58121.5117\n",
            "Epoch 339/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3692.6228 - mse: 3692.6228 - val_loss: 69178.7969 - val_mse: 69178.7969\n",
            "Epoch 340/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3421.1545 - mse: 3421.1545 - val_loss: 82316.2031 - val_mse: 82316.2031\n",
            "Epoch 341/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3029.1956 - mse: 3029.1956 - val_loss: 69403.1328 - val_mse: 69403.1328\n",
            "Epoch 342/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1803.8699 - mse: 1803.8699 - val_loss: 67872.1484 - val_mse: 67872.1484\n",
            "Epoch 343/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1761.7189 - mse: 1761.7189 - val_loss: 64048.5820 - val_mse: 64048.5820\n",
            "Epoch 344/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1906.2190 - mse: 1906.2190 - val_loss: 63996.1602 - val_mse: 63996.1602\n",
            "Epoch 345/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2271.9194 - mse: 2271.9194 - val_loss: 69626.1562 - val_mse: 69626.1562\n",
            "Epoch 346/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1992.4801 - mse: 1992.4801 - val_loss: 76044.3828 - val_mse: 76044.3828\n",
            "Epoch 347/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1643.5450 - mse: 1643.5450 - val_loss: 76747.9531 - val_mse: 76747.9531\n",
            "Epoch 348/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2127.0498 - mse: 2127.0498 - val_loss: 72759.5938 - val_mse: 72759.5938\n",
            "Epoch 349/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1587.9755 - mse: 1587.9755 - val_loss: 71003.7266 - val_mse: 71003.7266\n",
            "Epoch 350/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1544.8925 - mse: 1544.8925 - val_loss: 69302.0781 - val_mse: 69302.0781\n",
            "Epoch 351/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1349.7347 - mse: 1349.7347 - val_loss: 68074.8672 - val_mse: 68074.8672\n",
            "Epoch 352/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1271.0034 - mse: 1271.0034 - val_loss: 69919.2734 - val_mse: 69919.2734\n",
            "Epoch 353/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2009.1139 - mse: 2009.1139 - val_loss: 67761.4609 - val_mse: 67761.4609\n",
            "Epoch 354/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1780.0502 - mse: 1780.0502 - val_loss: 68218.1797 - val_mse: 68218.1797\n",
            "Epoch 355/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1270.3396 - mse: 1270.3396 - val_loss: 65408.9102 - val_mse: 65408.9102\n",
            "Epoch 356/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1340.3326 - mse: 1340.3326 - val_loss: 69174.8516 - val_mse: 69174.8516\n",
            "Epoch 357/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2311.5703 - mse: 2311.5703 - val_loss: 63485.3203 - val_mse: 63485.3203\n",
            "Epoch 358/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2363.0198 - mse: 2363.0198 - val_loss: 69530.0703 - val_mse: 69530.0703\n",
            "Epoch 359/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1166.2040 - mse: 1166.2040 - val_loss: 66592.9531 - val_mse: 66592.9531\n",
            "Epoch 360/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2601.8574 - mse: 2601.8574 - val_loss: 64582.4375 - val_mse: 64582.4375\n",
            "Epoch 361/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2845.1975 - mse: 2845.1975 - val_loss: 70571.7266 - val_mse: 70571.7266\n",
            "Epoch 362/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2010.6490 - mse: 2010.6490 - val_loss: 60204.8867 - val_mse: 60204.8867\n",
            "Epoch 363/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1639.9373 - mse: 1639.9373 - val_loss: 61000.8281 - val_mse: 61000.8281\n",
            "Epoch 364/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1437.9337 - mse: 1437.9337 - val_loss: 69665.4453 - val_mse: 69665.4453\n",
            "Epoch 365/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1815.2155 - mse: 1815.2155 - val_loss: 60216.8125 - val_mse: 60216.8125\n",
            "Epoch 366/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1532.1796 - mse: 1532.1796 - val_loss: 55746.6953 - val_mse: 55746.6953\n",
            "Epoch 367/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1199.3452 - mse: 1199.3452 - val_loss: 59317.3984 - val_mse: 59317.3984\n",
            "Epoch 368/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1419.4154 - mse: 1419.4154 - val_loss: 64499.1641 - val_mse: 64499.1641\n",
            "Epoch 369/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1429.9323 - mse: 1429.9323 - val_loss: 63093.7969 - val_mse: 63093.7969\n",
            "Epoch 370/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1460.3293 - mse: 1460.3293 - val_loss: 58096.6992 - val_mse: 58096.6992\n",
            "Epoch 371/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1256.8746 - mse: 1256.8746 - val_loss: 61657.4922 - val_mse: 61657.4922\n",
            "Epoch 372/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1178.2643 - mse: 1178.2643 - val_loss: 58692.7344 - val_mse: 58692.7344\n",
            "Epoch 373/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1211.7491 - mse: 1211.7491 - val_loss: 56925.7266 - val_mse: 56925.7266\n",
            "Epoch 374/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1364.7809 - mse: 1364.7809 - val_loss: 53407.4414 - val_mse: 53407.4414\n",
            "Epoch 375/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1263.6110 - mse: 1263.6110 - val_loss: 65838.2500 - val_mse: 65838.2500\n",
            "Epoch 376/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1172.4384 - mse: 1172.4384 - val_loss: 58339.1406 - val_mse: 58339.1406\n",
            "Epoch 377/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1087.4214 - mse: 1087.4214 - val_loss: 66423.5703 - val_mse: 66423.5703\n",
            "Epoch 378/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1202.7587 - mse: 1202.7587 - val_loss: 64960.6172 - val_mse: 64960.6172\n",
            "Epoch 379/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1158.6327 - mse: 1158.6327 - val_loss: 58565.0859 - val_mse: 58565.0859\n",
            "Epoch 380/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1185.1729 - mse: 1185.1729 - val_loss: 60526.1055 - val_mse: 60526.1055\n",
            "Epoch 381/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 990.0355 - mse: 990.0355 - val_loss: 56206.2109 - val_mse: 56206.2109\n",
            "Epoch 382/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1033.3826 - mse: 1033.3826 - val_loss: 54409.0156 - val_mse: 54409.0156\n",
            "Epoch 383/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1018.8260 - mse: 1018.8260 - val_loss: 58577.3047 - val_mse: 58577.3047\n",
            "Epoch 384/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2463.8452 - mse: 2463.8452 - val_loss: 55433.9961 - val_mse: 55433.9961\n",
            "Epoch 385/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1402.1523 - mse: 1402.1523 - val_loss: 66958.0703 - val_mse: 66958.0703\n",
            "Epoch 386/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4236.3833 - mse: 4236.3833 - val_loss: 61721.7305 - val_mse: 61721.7305\n",
            "Epoch 387/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3280.1882 - mse: 3280.1882 - val_loss: 57487.7109 - val_mse: 57487.7109\n",
            "Epoch 388/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1820.3723 - mse: 1820.3723 - val_loss: 66476.3984 - val_mse: 66476.3984\n",
            "Epoch 389/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1427.4755 - mse: 1427.4755 - val_loss: 55988.3516 - val_mse: 55988.3516\n",
            "Epoch 390/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1206.6576 - mse: 1206.6576 - val_loss: 55102.5664 - val_mse: 55102.5664\n",
            "Epoch 391/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1274.7375 - mse: 1274.7375 - val_loss: 49521.4023 - val_mse: 49521.4023\n",
            "Epoch 392/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1032.1510 - mse: 1032.1510 - val_loss: 54696.9023 - val_mse: 54696.9023\n",
            "Epoch 393/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1027.7936 - mse: 1027.7936 - val_loss: 53582.7109 - val_mse: 53582.7109\n",
            "Epoch 394/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 966.2429 - mse: 966.2429 - val_loss: 52283.4805 - val_mse: 52283.4805\n",
            "Epoch 395/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 885.9803 - mse: 885.9803 - val_loss: 57296.2109 - val_mse: 57296.2109\n",
            "Epoch 396/600\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 906.3353 - mse: 906.3353 - val_loss: 58801.5586 - val_mse: 58801.5586\n",
            "Epoch 397/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 859.1416 - mse: 859.1416 - val_loss: 57342.8242 - val_mse: 57342.8242\n",
            "Epoch 398/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 947.6890 - mse: 947.6890 - val_loss: 53942.5117 - val_mse: 53942.5117\n",
            "Epoch 399/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 987.1198 - mse: 987.1198 - val_loss: 54309.2383 - val_mse: 54309.2383\n",
            "Epoch 400/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1280.7993 - mse: 1280.7993 - val_loss: 52774.0742 - val_mse: 52774.0742\n",
            "Epoch 401/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1262.7358 - mse: 1262.7358 - val_loss: 51439.0234 - val_mse: 51439.0234\n",
            "Epoch 402/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2293.4226 - mse: 2293.4226 - val_loss: 52003.6836 - val_mse: 52003.6836\n",
            "Epoch 403/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2077.6003 - mse: 2077.6003 - val_loss: 45621.9375 - val_mse: 45621.9375\n",
            "Epoch 404/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2982.0076 - mse: 2982.0076 - val_loss: 61160.7695 - val_mse: 61160.7695\n",
            "Epoch 405/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2776.6597 - mse: 2776.6597 - val_loss: 41693.3711 - val_mse: 41693.3711\n",
            "Epoch 406/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3016.3018 - mse: 3016.3018 - val_loss: 41387.5039 - val_mse: 41387.5039\n",
            "Epoch 407/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4195.2295 - mse: 4195.2295 - val_loss: 69404.7031 - val_mse: 69404.7031\n",
            "Epoch 408/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3892.2319 - mse: 3892.2319 - val_loss: 46837.3477 - val_mse: 46837.3477\n",
            "Epoch 409/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3798.9397 - mse: 3798.9397 - val_loss: 49210.0000 - val_mse: 49210.0000\n",
            "Epoch 410/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 4886.9365 - mse: 4886.9365 - val_loss: 52133.2344 - val_mse: 52133.2344\n",
            "Epoch 411/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 5699.9146 - mse: 5699.9146 - val_loss: 60941.4414 - val_mse: 60941.4414\n",
            "Epoch 412/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 8413.4287 - mse: 8413.4287 - val_loss: 44519.8086 - val_mse: 44519.8086\n",
            "Epoch 413/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 5225.3296 - mse: 5225.3296 - val_loss: 51102.0586 - val_mse: 51102.0586\n",
            "Epoch 414/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2361.9297 - mse: 2361.9297 - val_loss: 43235.2461 - val_mse: 43235.2461\n",
            "Epoch 415/600\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 34018.3047 - mse: 34018.3047 - val_loss: 43307.8516 - val_mse: 43307.8516\n",
            "Epoch 416/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 4492.7070 - mse: 4492.7070 - val_loss: 55984.9922 - val_mse: 55984.9922\n",
            "Epoch 417/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 16111.4297 - mse: 16111.4297 - val_loss: 54399.8125 - val_mse: 54399.8125\n",
            "Epoch 418/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3607.2244 - mse: 3607.2244 - val_loss: 53071.9219 - val_mse: 53071.9219\n",
            "Epoch 419/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2888.4395 - mse: 2888.4395 - val_loss: 52431.5742 - val_mse: 52431.5742\n",
            "Epoch 420/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2195.5156 - mse: 2195.5156 - val_loss: 66117.7812 - val_mse: 66117.7812\n",
            "Epoch 421/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2785.8733 - mse: 2785.8733 - val_loss: 55084.5430 - val_mse: 55084.5430\n",
            "Epoch 422/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1992.9285 - mse: 1992.9285 - val_loss: 56180.3516 - val_mse: 56180.3516\n",
            "Epoch 423/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1253.0607 - mse: 1253.0607 - val_loss: 50184.4531 - val_mse: 50184.4531\n",
            "Epoch 424/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1524.0586 - mse: 1524.0586 - val_loss: 56327.5352 - val_mse: 56327.5352\n",
            "Epoch 425/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 886.8989 - mse: 886.8989 - val_loss: 58512.4922 - val_mse: 58512.4922\n",
            "Epoch 426/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1002.3860 - mse: 1002.3860 - val_loss: 52490.9883 - val_mse: 52490.9883\n",
            "Epoch 427/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 826.0018 - mse: 826.0018 - val_loss: 58560.1914 - val_mse: 58560.1914\n",
            "Epoch 428/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 751.5626 - mse: 751.5626 - val_loss: 53593.2617 - val_mse: 53593.2617\n",
            "Epoch 429/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 753.9507 - mse: 753.9507 - val_loss: 55766.5820 - val_mse: 55766.5820\n",
            "Epoch 430/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 877.5458 - mse: 877.5458 - val_loss: 51579.0430 - val_mse: 51579.0430\n",
            "Epoch 431/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 996.8561 - mse: 996.8561 - val_loss: 50981.9492 - val_mse: 50981.9492\n",
            "Epoch 432/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 904.5734 - mse: 904.5734 - val_loss: 54764.7070 - val_mse: 54764.7070\n",
            "Epoch 433/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 980.5266 - mse: 980.5266 - val_loss: 50333.7266 - val_mse: 50333.7266\n",
            "Epoch 434/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 770.3018 - mse: 770.3018 - val_loss: 52317.6250 - val_mse: 52317.6250\n",
            "Epoch 435/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1039.3907 - mse: 1039.3907 - val_loss: 44968.2656 - val_mse: 44968.2656\n",
            "Epoch 436/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 665.8645 - mse: 665.8645 - val_loss: 45613.1758 - val_mse: 45613.1758\n",
            "Epoch 437/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1838.5460 - mse: 1838.5460 - val_loss: 44682.6953 - val_mse: 44682.6953\n",
            "Epoch 438/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1323.8700 - mse: 1323.8700 - val_loss: 52187.0000 - val_mse: 52187.0000\n",
            "Epoch 439/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 894.2870 - mse: 894.2870 - val_loss: 41426.3906 - val_mse: 41426.3906\n",
            "Epoch 440/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1389.8270 - mse: 1389.8270 - val_loss: 42082.5625 - val_mse: 42082.5625\n",
            "Epoch 441/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1353.2551 - mse: 1353.2551 - val_loss: 47484.0391 - val_mse: 47484.0391\n",
            "Epoch 442/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1128.6416 - mse: 1128.6416 - val_loss: 48543.5391 - val_mse: 48543.5391\n",
            "Epoch 443/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1258.6366 - mse: 1258.6366 - val_loss: 49465.7578 - val_mse: 49465.7578\n",
            "Epoch 444/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1317.0319 - mse: 1317.0319 - val_loss: 57788.4453 - val_mse: 57788.4453\n",
            "Epoch 445/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1143.8988 - mse: 1143.8988 - val_loss: 46942.7383 - val_mse: 46942.7383\n",
            "Epoch 446/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1014.5887 - mse: 1014.5887 - val_loss: 38207.9258 - val_mse: 38207.9258\n",
            "Epoch 447/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 858.0490 - mse: 858.0490 - val_loss: 42326.4961 - val_mse: 42326.4961\n",
            "Epoch 448/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 747.6660 - mse: 747.6660 - val_loss: 38474.4883 - val_mse: 38474.4883\n",
            "Epoch 449/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 746.1780 - mse: 746.1780 - val_loss: 42021.5859 - val_mse: 42021.5859\n",
            "Epoch 450/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 703.2012 - mse: 703.2012 - val_loss: 38207.7969 - val_mse: 38207.7969\n",
            "Epoch 451/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1301.1663 - mse: 1301.1663 - val_loss: 43769.8867 - val_mse: 43769.8867\n",
            "Epoch 452/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 929.0595 - mse: 929.0595 - val_loss: 40452.6133 - val_mse: 40452.6133\n",
            "Epoch 453/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 833.4341 - mse: 833.4341 - val_loss: 39831.5664 - val_mse: 39831.5664\n",
            "Epoch 454/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 772.9584 - mse: 772.9584 - val_loss: 40935.1797 - val_mse: 40935.1797\n",
            "Epoch 455/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 696.8961 - mse: 696.8961 - val_loss: 42222.3008 - val_mse: 42222.3008\n",
            "Epoch 456/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 959.3411 - mse: 959.3411 - val_loss: 42445.5625 - val_mse: 42445.5625\n",
            "Epoch 457/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 694.9941 - mse: 694.9941 - val_loss: 46920.5430 - val_mse: 46920.5430\n",
            "Epoch 458/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 992.8399 - mse: 992.8399 - val_loss: 46877.9844 - val_mse: 46877.9844\n",
            "Epoch 459/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1096.6940 - mse: 1096.6940 - val_loss: 42201.5078 - val_mse: 42201.5078\n",
            "Epoch 460/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 977.5228 - mse: 977.5228 - val_loss: 42938.7656 - val_mse: 42938.7656\n",
            "Epoch 461/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 987.7979 - mse: 987.7979 - val_loss: 49064.9844 - val_mse: 49064.9844\n",
            "Epoch 462/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 880.0312 - mse: 880.0312 - val_loss: 45799.2500 - val_mse: 45799.2500\n",
            "Epoch 463/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 822.8151 - mse: 822.8151 - val_loss: 45491.8750 - val_mse: 45491.8750\n",
            "Epoch 464/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 784.9056 - mse: 784.9056 - val_loss: 41913.8398 - val_mse: 41913.8398\n",
            "Epoch 465/600\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 799.9574 - mse: 799.9574 - val_loss: 42314.8750 - val_mse: 42314.8750\n",
            "Epoch 466/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 658.0289 - mse: 658.0289 - val_loss: 42354.3789 - val_mse: 42354.3789\n",
            "Epoch 467/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 660.3658 - mse: 660.3658 - val_loss: 41634.9297 - val_mse: 41634.9297\n",
            "Epoch 468/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 654.4506 - mse: 654.4506 - val_loss: 38212.1914 - val_mse: 38212.1914\n",
            "Epoch 469/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 671.2332 - mse: 671.2332 - val_loss: 38571.9883 - val_mse: 38571.9883\n",
            "Epoch 470/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 710.5535 - mse: 710.5535 - val_loss: 36436.6250 - val_mse: 36436.6250\n",
            "Epoch 471/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 657.5025 - mse: 657.5025 - val_loss: 36163.1797 - val_mse: 36163.1797\n",
            "Epoch 472/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 739.6038 - mse: 739.6038 - val_loss: 38199.6914 - val_mse: 38199.6914\n",
            "Epoch 473/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 637.1270 - mse: 637.1270 - val_loss: 36339.8359 - val_mse: 36339.8359\n",
            "Epoch 474/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 637.5653 - mse: 637.5653 - val_loss: 30197.5508 - val_mse: 30197.5508\n",
            "Epoch 475/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 714.5264 - mse: 714.5264 - val_loss: 38805.2969 - val_mse: 38805.2969\n",
            "Epoch 476/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 806.7333 - mse: 806.7333 - val_loss: 31928.4238 - val_mse: 31928.4238\n",
            "Epoch 477/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1420.0908 - mse: 1420.0908 - val_loss: 40566.9531 - val_mse: 40566.9531\n",
            "Epoch 478/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2089.6638 - mse: 2089.6638 - val_loss: 31978.1387 - val_mse: 31978.1387\n",
            "Epoch 479/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2051.5020 - mse: 2051.5020 - val_loss: 37091.0273 - val_mse: 37091.0273\n",
            "Epoch 480/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2187.9856 - mse: 2187.9856 - val_loss: 30659.5664 - val_mse: 30659.5664\n",
            "Epoch 481/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1896.9722 - mse: 1896.9722 - val_loss: 44246.4570 - val_mse: 44246.4570\n",
            "Epoch 482/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1358.1324 - mse: 1358.1324 - val_loss: 34297.8320 - val_mse: 34297.8320\n",
            "Epoch 483/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1092.4384 - mse: 1092.4384 - val_loss: 42738.1328 - val_mse: 42738.1328\n",
            "Epoch 484/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 901.5385 - mse: 901.5385 - val_loss: 40628.4609 - val_mse: 40628.4609\n",
            "Epoch 485/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 890.9891 - mse: 890.9891 - val_loss: 41528.4766 - val_mse: 41528.4766\n",
            "Epoch 486/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 770.8350 - mse: 770.8350 - val_loss: 40306.4258 - val_mse: 40306.4258\n",
            "Epoch 487/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1097.0994 - mse: 1097.0994 - val_loss: 41303.1445 - val_mse: 41303.1445\n",
            "Epoch 488/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 892.6934 - mse: 892.6934 - val_loss: 38792.6680 - val_mse: 38792.6680\n",
            "Epoch 489/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 693.8030 - mse: 693.8030 - val_loss: 37350.7539 - val_mse: 37350.7539\n",
            "Epoch 490/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1014.3035 - mse: 1014.3035 - val_loss: 39211.4453 - val_mse: 39211.4453\n",
            "Epoch 491/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 886.9254 - mse: 886.9254 - val_loss: 32731.2695 - val_mse: 32731.2695\n",
            "Epoch 492/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3244.3313 - mse: 3244.3313 - val_loss: 40816.5859 - val_mse: 40816.5859\n",
            "Epoch 493/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 2924.2273 - mse: 2924.2273 - val_loss: 36047.2148 - val_mse: 36047.2148\n",
            "Epoch 494/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1434.9832 - mse: 1434.9832 - val_loss: 38598.9219 - val_mse: 38598.9219\n",
            "Epoch 495/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1215.6338 - mse: 1215.6338 - val_loss: 43531.0078 - val_mse: 43531.0078\n",
            "Epoch 496/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2249.7439 - mse: 2249.7439 - val_loss: 35652.0430 - val_mse: 35652.0430\n",
            "Epoch 497/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2899.1760 - mse: 2899.1760 - val_loss: 46792.3047 - val_mse: 46792.3047\n",
            "Epoch 498/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 6572.2812 - mse: 6572.2812 - val_loss: 34061.7656 - val_mse: 34061.7656\n",
            "Epoch 499/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 3078.4866 - mse: 3078.4866 - val_loss: 42297.3672 - val_mse: 42297.3672\n",
            "Epoch 500/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3177.3687 - mse: 3177.3687 - val_loss: 30676.6328 - val_mse: 30676.6328\n",
            "Epoch 501/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2804.7842 - mse: 2804.7842 - val_loss: 22643.7754 - val_mse: 22643.7754\n",
            "Epoch 502/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2857.2620 - mse: 2857.2620 - val_loss: 34062.5898 - val_mse: 34062.5898\n",
            "Epoch 503/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2476.8755 - mse: 2476.8755 - val_loss: 34966.8398 - val_mse: 34966.8398\n",
            "Epoch 504/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2491.0017 - mse: 2491.0017 - val_loss: 32462.5234 - val_mse: 32462.5234\n",
            "Epoch 505/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4277.7471 - mse: 4277.7471 - val_loss: 36373.7266 - val_mse: 36373.7266\n",
            "Epoch 506/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4701.5894 - mse: 4701.5894 - val_loss: 44162.4375 - val_mse: 44162.4375\n",
            "Epoch 507/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3605.2715 - mse: 3605.2715 - val_loss: 28365.3418 - val_mse: 28365.3418\n",
            "Epoch 508/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4108.6958 - mse: 4108.6958 - val_loss: 41426.4766 - val_mse: 41426.4766\n",
            "Epoch 509/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2713.0415 - mse: 2713.0415 - val_loss: 30687.5273 - val_mse: 30687.5273\n",
            "Epoch 510/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2373.0840 - mse: 2373.0840 - val_loss: 32360.7559 - val_mse: 32360.7559\n",
            "Epoch 511/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1919.6858 - mse: 1919.6858 - val_loss: 25657.3418 - val_mse: 25657.3418\n",
            "Epoch 512/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1800.8689 - mse: 1800.8689 - val_loss: 41363.7617 - val_mse: 41363.7617\n",
            "Epoch 513/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1477.1367 - mse: 1477.1367 - val_loss: 35211.1172 - val_mse: 35211.1172\n",
            "Epoch 514/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 996.6096 - mse: 996.6096 - val_loss: 39216.4414 - val_mse: 39216.4414\n",
            "Epoch 515/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1280.2709 - mse: 1280.2709 - val_loss: 33309.0586 - val_mse: 33309.0586\n",
            "Epoch 516/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 787.3663 - mse: 787.3663 - val_loss: 42641.2773 - val_mse: 42641.2773\n",
            "Epoch 517/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1324.7467 - mse: 1324.7467 - val_loss: 31087.2695 - val_mse: 31087.2695\n",
            "Epoch 518/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1703.2601 - mse: 1703.2601 - val_loss: 37463.5859 - val_mse: 37463.5859\n",
            "Epoch 519/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1121.7314 - mse: 1121.7314 - val_loss: 24944.7383 - val_mse: 24944.7383\n",
            "Epoch 520/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1029.5444 - mse: 1029.5444 - val_loss: 32553.4238 - val_mse: 32553.4238\n",
            "Epoch 521/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1130.5311 - mse: 1130.5311 - val_loss: 31578.5801 - val_mse: 31578.5801\n",
            "Epoch 522/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1130.8937 - mse: 1130.8937 - val_loss: 31049.4727 - val_mse: 31049.4727\n",
            "Epoch 523/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 762.5564 - mse: 762.5564 - val_loss: 28321.7871 - val_mse: 28321.7871\n",
            "Epoch 524/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 552.8566 - mse: 552.8566 - val_loss: 30691.9707 - val_mse: 30691.9707\n",
            "Epoch 525/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 416.6976 - mse: 416.6976 - val_loss: 27576.5117 - val_mse: 27576.5117\n",
            "Epoch 526/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 475.3966 - mse: 475.3966 - val_loss: 27957.3633 - val_mse: 27957.3633\n",
            "Epoch 527/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 512.0134 - mse: 512.0134 - val_loss: 30542.1699 - val_mse: 30542.1699\n",
            "Epoch 528/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 667.6145 - mse: 667.6145 - val_loss: 30826.7422 - val_mse: 30826.7422\n",
            "Epoch 529/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 684.2650 - mse: 684.2650 - val_loss: 25387.4375 - val_mse: 25387.4375\n",
            "Epoch 530/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 724.7706 - mse: 724.7706 - val_loss: 29442.0820 - val_mse: 29442.0820\n",
            "Epoch 531/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1111.9799 - mse: 1111.9799 - val_loss: 28972.7168 - val_mse: 28972.7168\n",
            "Epoch 532/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4538.6562 - mse: 4538.6562 - val_loss: 28709.7930 - val_mse: 28709.7930\n",
            "Epoch 533/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1882.1877 - mse: 1882.1877 - val_loss: 31131.7559 - val_mse: 31131.7559\n",
            "Epoch 534/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4891.2100 - mse: 4891.2100 - val_loss: 37411.0234 - val_mse: 37411.0234\n",
            "Epoch 535/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1965.8440 - mse: 1965.8440 - val_loss: 19909.9082 - val_mse: 19909.9082\n",
            "Epoch 536/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 9443.0801 - mse: 9443.0801 - val_loss: 23691.8750 - val_mse: 23691.8750\n",
            "Epoch 537/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1968.3208 - mse: 1968.3208 - val_loss: 32273.2227 - val_mse: 32273.2227\n",
            "Epoch 538/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1738.4696 - mse: 1738.4696 - val_loss: 29369.3965 - val_mse: 29369.3965\n",
            "Epoch 539/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 996.3758 - mse: 996.3758 - val_loss: 28322.6641 - val_mse: 28322.6641\n",
            "Epoch 540/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1424.5636 - mse: 1424.5636 - val_loss: 26873.9570 - val_mse: 26873.9570\n",
            "Epoch 541/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1064.7710 - mse: 1064.7710 - val_loss: 36695.9648 - val_mse: 36695.9648\n",
            "Epoch 542/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1354.9839 - mse: 1354.9839 - val_loss: 25555.4961 - val_mse: 25555.4961\n",
            "Epoch 543/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 952.6434 - mse: 952.6434 - val_loss: 28813.7305 - val_mse: 28813.7305\n",
            "Epoch 544/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1228.6842 - mse: 1228.6842 - val_loss: 27838.5098 - val_mse: 27838.5098\n",
            "Epoch 545/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1266.6931 - mse: 1266.6931 - val_loss: 24882.2676 - val_mse: 24882.2676\n",
            "Epoch 546/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1913.7159 - mse: 1913.7159 - val_loss: 28525.2832 - val_mse: 28525.2832\n",
            "Epoch 547/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2136.5107 - mse: 2136.5107 - val_loss: 35568.6250 - val_mse: 35568.6250\n",
            "Epoch 548/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1335.5496 - mse: 1335.5496 - val_loss: 30967.9336 - val_mse: 30967.9336\n",
            "Epoch 549/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1125.7373 - mse: 1125.7373 - val_loss: 21050.9805 - val_mse: 21050.9805\n",
            "Epoch 550/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1273.1423 - mse: 1273.1423 - val_loss: 26413.7930 - val_mse: 26413.7930\n",
            "Epoch 551/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1841.2606 - mse: 1841.2606 - val_loss: 25961.7734 - val_mse: 25961.7734\n",
            "Epoch 552/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1766.0338 - mse: 1766.0338 - val_loss: 21772.1367 - val_mse: 21772.1367\n",
            "Epoch 553/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3480.1333 - mse: 3480.1333 - val_loss: 18890.3574 - val_mse: 18890.3574\n",
            "Epoch 554/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2805.9226 - mse: 2805.9226 - val_loss: 35252.6016 - val_mse: 35252.6016\n",
            "Epoch 555/600\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 2117.0974 - mse: 2117.0974 - val_loss: 23349.3320 - val_mse: 23349.3320\n",
            "Epoch 556/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1128.2557 - mse: 1128.2557 - val_loss: 21131.0410 - val_mse: 21131.0410\n",
            "Epoch 557/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1697.6837 - mse: 1697.6837 - val_loss: 21527.9043 - val_mse: 21527.9043\n",
            "Epoch 558/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2052.3706 - mse: 2052.3706 - val_loss: 25750.9961 - val_mse: 25750.9961\n",
            "Epoch 559/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1821.8184 - mse: 1821.8184 - val_loss: 22360.3457 - val_mse: 22360.3457\n",
            "Epoch 560/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1986.9368 - mse: 1986.9368 - val_loss: 22512.1836 - val_mse: 22512.1836\n",
            "Epoch 561/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2232.8352 - mse: 2232.8352 - val_loss: 20272.9609 - val_mse: 20272.9609\n",
            "Epoch 562/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1884.3068 - mse: 1884.3068 - val_loss: 26569.1152 - val_mse: 26569.1152\n",
            "Epoch 563/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1516.1731 - mse: 1516.1731 - val_loss: 28007.5371 - val_mse: 28007.5371\n",
            "Epoch 564/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1166.4661 - mse: 1166.4661 - val_loss: 22458.3125 - val_mse: 22458.3125\n",
            "Epoch 565/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 918.9718 - mse: 918.9718 - val_loss: 22768.9473 - val_mse: 22768.9473\n",
            "Epoch 566/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1578.6337 - mse: 1578.6337 - val_loss: 23243.3516 - val_mse: 23243.3516\n",
            "Epoch 567/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4670.2588 - mse: 4670.2588 - val_loss: 26300.3301 - val_mse: 26300.3301\n",
            "Epoch 568/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1669.3196 - mse: 1669.3196 - val_loss: 19376.2637 - val_mse: 19376.2637\n",
            "Epoch 569/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 3345.5359 - mse: 3345.5359 - val_loss: 32160.8965 - val_mse: 32160.8965\n",
            "Epoch 570/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2793.0300 - mse: 2793.0300 - val_loss: 27552.2637 - val_mse: 27552.2637\n",
            "Epoch 571/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 4793.6128 - mse: 4793.6128 - val_loss: 22065.6465 - val_mse: 22065.6465\n",
            "Epoch 572/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3195.4458 - mse: 3195.4458 - val_loss: 24395.8750 - val_mse: 24395.8750\n",
            "Epoch 573/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 2026.0437 - mse: 2026.0437 - val_loss: 33772.3281 - val_mse: 33772.3281\n",
            "Epoch 574/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 3984.7329 - mse: 3984.7329 - val_loss: 19760.2949 - val_mse: 19760.2949\n",
            "Epoch 575/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 2400.2388 - mse: 2400.2388 - val_loss: 23862.5859 - val_mse: 23862.5859\n",
            "Epoch 576/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2609.4287 - mse: 2609.4287 - val_loss: 29372.0762 - val_mse: 29372.0762\n",
            "Epoch 577/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1909.0900 - mse: 1909.0900 - val_loss: 29477.5645 - val_mse: 29477.5645\n",
            "Epoch 578/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1030.4896 - mse: 1030.4896 - val_loss: 24931.2402 - val_mse: 24931.2402\n",
            "Epoch 579/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 919.3568 - mse: 919.3568 - val_loss: 27342.4941 - val_mse: 27342.4941\n",
            "Epoch 580/600\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 964.1484 - mse: 964.1484 - val_loss: 26986.8086 - val_mse: 26986.8086\n",
            "Epoch 581/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1219.6982 - mse: 1219.6982 - val_loss: 21457.6895 - val_mse: 21457.6895\n",
            "Epoch 582/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1232.9631 - mse: 1232.9631 - val_loss: 22430.5547 - val_mse: 22430.5547\n",
            "Epoch 583/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3966.1003 - mse: 3966.1003 - val_loss: 31496.9629 - val_mse: 31496.9629\n",
            "Epoch 584/600\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1429.3384 - mse: 1429.3384 - val_loss: 29524.2559 - val_mse: 29524.2559\n",
            "Epoch 585/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 2340.7510 - mse: 2340.7510 - val_loss: 30625.9473 - val_mse: 30625.9473\n",
            "Epoch 586/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1744.7195 - mse: 1744.7195 - val_loss: 26304.3105 - val_mse: 26304.3105\n",
            "Epoch 587/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1172.0929 - mse: 1172.0929 - val_loss: 25571.9180 - val_mse: 25571.9180\n",
            "Epoch 588/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1127.6300 - mse: 1127.6300 - val_loss: 28056.8633 - val_mse: 28056.8633\n",
            "Epoch 589/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 800.7607 - mse: 800.7607 - val_loss: 22363.7109 - val_mse: 22363.7109\n",
            "Epoch 590/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 806.4874 - mse: 806.4874 - val_loss: 25634.3652 - val_mse: 25634.3652\n",
            "Epoch 591/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 815.4597 - mse: 815.4597 - val_loss: 22125.1016 - val_mse: 22125.1016\n",
            "Epoch 592/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 883.0333 - mse: 883.0333 - val_loss: 24098.1934 - val_mse: 24098.1934\n",
            "Epoch 593/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 708.9781 - mse: 708.9781 - val_loss: 22181.5703 - val_mse: 22181.5703\n",
            "Epoch 594/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 707.2133 - mse: 707.2133 - val_loss: 21769.3574 - val_mse: 21769.3574\n",
            "Epoch 595/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 534.0971 - mse: 534.0971 - val_loss: 23243.2598 - val_mse: 23243.2598\n",
            "Epoch 596/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 586.6269 - mse: 586.6269 - val_loss: 22878.6270 - val_mse: 22878.6270\n",
            "Epoch 597/600\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 534.2014 - mse: 534.2014 - val_loss: 23505.9961 - val_mse: 23505.9961\n",
            "Epoch 598/600\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 457.5955 - mse: 457.5955 - val_loss: 27722.7910 - val_mse: 27722.7910\n",
            "Epoch 599/600\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 399.6918 - mse: 399.6918 - val_loss: 23279.7246 - val_mse: 23279.7246\n",
            "Epoch 600/600\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 353.9699 - mse: 353.9699 - val_loss: 24760.6172 - val_mse: 24760.6172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictionstotal = modeltotal.predict(samplestotaltest)\n",
        "predictionstotaltrain = modeltotal.predict(samplestotal) #Ook eens testen op traindata voor controle\n",
        "predictionstotalval = modeltotal.predict(samplestotalvalidation) #Ook eens testen op validatiedata voor controle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJlWM3lo-T0M",
        "outputId": "199f59af-05ce-4470-c798-7c56e736b2ed"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 4ms/step\n",
            "9/9 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotten van de errors op testdata\n",
        "predictionslisttotal = predictionstotal.tolist()  #converteren naar lijst\n",
        "predictionslisttotalflat = [item for sublist in predictionslisttotal for item in sublist] #lijst plat maken\n",
        "\n",
        "delaypiektotaltest = delaypiekASDRtest + delaypiekGAUSStest + delaypiekSBOXtest\n",
        "plotpredictieerrors(predictionslisttotalflat, delaypiektotaltest, \"totaltestset\", \"totaltest\")\n",
        "\n",
        "\n",
        "#plotten van de errors op traindata\n",
        "predictionslisttotaltrain = predictionstotaltrain.tolist()\n",
        "predictionslisttotaltrainflat = [item for sublist in predictionslisttotaltrain for item in sublist] #lijst plat maken\n",
        "\n",
        "delaytotal = delaypiekASDR[0:90] + delaypiekGAUSS[0:90] + delaypiekSBOX[0:90]\n",
        "plotpredictieerrors(predictionslisttotaltrainflat, delaytotal, \"totaltrainset\", 'totaltrain')\n",
        "\n",
        "\n",
        "#plotten van de errors op validatiedata\n",
        "predictionslisttotalval = predictionstotalval.tolist()\n",
        "predictionslisttotalvalflat = [item for sublist in predictionslisttotalval for item in sublist] #lijst plat maken\n",
        "\n",
        "delaytotalvalidation = delaypiekASDR[90:] + delaypiekGAUSS[90:] + delaypiekSBOX[90:]\n",
        "\n",
        "plotpredictieerrors(predictionslisttotalvalflat, delaytotalvalidation, \"totalvalidatieset\", 'totalvalidation')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vWvtI8Jd-n4c",
        "outputId": "1f7e84ad-b483-4456-91ae-ebb9a7af34cc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De errors zijn (afgerond tot gehele waarden):\n",
            "[77, 250, 25, 2, 4, 40, 52, 16, 12, 30, 107, 31, 52, 92, 5, 11, 38, 51, 104, 40, 94, 94, 81, 148, 11, 24, 107, 5, 55, 73, 54, 9, 52, 56, 0, 179, 20, 73, 62, 112, 199, 5, 2, 64, 94, 79, 28, 56, 22, 41, 19, 69, 70, 26, 57, 13, 4, 44, 30, 86, 78, 58, 45, 57, 85, 76, 34, 157, 53, 24, 7, 80, 47, 67, 38, 103, 3, 66, 108, 45, 55, 14, 77, 47, 57, 100, 24, 54, 120, 46, 42, 28, 11, 76, 80, 88, 57, 291, 130, 21, 281, 51, 19, 24, 4, 26, 46, 75, 35, 78, 48, 10, 210, 6, 231, 594, 243, 61, 24, 29, 30, 93, 25, 147, 116, 38, 152, 143, 133, 9, 29, 245, 39, 209, 40, 76, 88, 284, 35, 114, 162, 95, 69, 41, 46, 18, 129, 1817, 4225, 568, 14, 125, 37, 65, 4192, 70, 39, 140, 352, 13, 15, 324, 82, 11, 12, 218, 107, 65, 14, 438, 63, 97, 795, 51, 583, 74, 356, 69, 16, 34, 98, 567, 243, 262, 95, 124, 115, 12, 132, 223, 24, 156, 30, 82, 99, 970, 12, 401, 30, 74, 88, 8, 16, 28, 58, 98, 97, 32, 65, 62, 29, 42, 130, 104, 18, 44, 3, 28, 7, 48, 40, 1, 40, 23, 104, 1, 4, 5, 37, 24, 9, 13, 3, 32, 41, 73, 18, 3, 29, 5, 76, 14, 3, 34, 74, 7, 13, 39, 18, 8, 3, 32, 121, 90, 14, 42, 111, 4, 20, 26, 38, 22, 55, 62, 40, 109, 51, 28, 8, 5, 26, 21, 3, 5, 15, 5, 63, 91, 8, 25, 12, 101, 72, 32, 68, 52, 6, 11, 4, 38, 101, 3, 25, 71, 93, 26, 4, 24, 40, 41]\n",
            "De predicties zijn:\n",
            "[520.353271484375, 355.6620788574219, 527.2813720703125, 601.4821166992188, 562.7674560546875, 520.209716796875, 614.649169921875, 527.0435180664062, 572.1410522460938, 572.2640380859375, 504.4657287597656, 598.6036987304688, 622.973876953125, 482.2552490234375, 552.3287963867188, 553.3519897460938, 645.2437744140625, 495.96978759765625, 483.172119140625, 536.2994995117188, 495.37548828125, 505.4651794433594, 526.556640625, 450.0023498535156, 537.4940795898438, 522.732421875, 504.5588073730469, 559.6920776367188, 493.5575256347656, 475.5335998535156, 599.9335327148438, 571.1617431640625, 492.5420837402344, 636.2182006835938, 551.9937744140625, 433.0134582519531, 550.6541748046875, 481.29278564453125, 518.5059204101562, 499.1116943359375, 402.15911865234375, 551.59814453125, 553.8944091796875, 545.3612060546875, 676.9832763671875, 510.9893493652344, 570.34521484375, 497.1338195800781, 532.7658081054688, 557.8471069335938, 578.3673095703125, 529.58154296875, 507.2356872558594, 543.5660400390625, 529.0916137695312, 574.1749267578125, 558.7420654296875, 531.0335693359375, 626.6785888671875, 457.8330383300781, 489.86932373046875, 524.2037963867188, 497.27655029296875, 501.3347473144531, 466.59918212890625, 506.8267517089844, 533.0325927734375, 439.12060546875, 494.1746520996094, 616.7337646484375, 584.9376220703125, 528.2738647460938, 512.9759521484375, 615.1258544921875, 524.0734252929688, 505.0452575683594, 561.8135986328125, 527.03271484375, 454.3591613769531, 523.58349609375, 553.3106079101562, 542.382080078125, 487.3649597167969, 598.404541015625, 639.79052734375, 468.7330017089844, 528.9874267578125, 535.0280151367188, 483.7921142578125, 522.337646484375, 515.6724853515625, 621.0579223632812, 548.3074951171875, 512.427734375, 519.3052368164062, 517.6832275390625, 517.3720092773438, 297.4891662597656, 433.7480773925781, 579.2110595703125, 833.0078735351562, 640.0028686523438, 571.7852172851562, 628.145751953125, 557.9373779296875, 581.58251953125, 642.7545776367188, 629.1741333007812, 576.2042846679688, 663.768310546875, 626.684814453125, 585.0119018554688, 379.1477355957031, 611.0277099609375, 373.9185791015625, -5.8760199546813965, 830.0673828125, 487.2033386230469, 531.0939331054688, 579.6995239257812, 573.529052734375, 698.2704467773438, 537.9337158203125, 705.739990234375, 674.4991455078125, 508.7921142578125, 753.0810546875, 749.3875122070312, 739.9947509765625, 564.4964599609375, 516.27392578125, 811.4398803710938, 643.0497436523438, 349.7223815917969, 623.1171264648438, 684.721435546875, 508.8473205566406, 894.9363403320312, 642.7052001953125, 690.4871826171875, 753.1215209960938, 668.2398681640625, 664.1213989257812, 559.2466430664062, 645.369140625, 621.7911987304688, 428.47515869140625, -1255.558349609375, 4787.2158203125, 31.112661361694336, 530.0629272460938, 684.76708984375, 642.0101318359375, 509.2659606933594, 4803.189453125, 659.583740234375, 635.1439819335938, 720.4849853515625, 199.18421936035156, 591.9122314453125, 614.1578369140625, 919.6361083984375, 651.9227294921875, 583.2883911132812, 577.4947509765625, 823.2192993164062, 695.96875, 541.4783325195312, 610.8031005859375, 1033.9349365234375, 655.3704833984375, 449.029541015625, 1360.3323974609375, 607.3077392578125, 1162.6807861328125, 474.6191711425781, 209.40687561035156, 673.6273193359375, 613.06005859375, 638.705810546875, 704.7689819335938, -0.02234521508216858, 793.434326171875, 858.0497436523438, 685.81591796875, 682.9052734375, 723.679931640625, 531.161376953125, 673.5237426757812, 801.346435546875, 517.9813232421875, 711.7180786132812, 641.6876220703125, 653.079833984375, 685.1597900390625, -366.2149963378906, 559.7977294921875, 979.2588500976562, 624.158935546875, 644.7694702148438, 494.608642578125, 552.681640625, 570.380859375, 519.9218139648438, 502.1540832519531, 647.2858276367188, 476.3717956542969, 588.0859985351562, 623.395263671875, 645.1123046875, 556.787109375, 544.775146484375, 688.320068359375, 671.1073608398438, 572.95458984375, 551.89013671875, 539.2960205078125, 602.9365844726562, 551.0303955078125, 500.94586181640625, 567.6126098632812, 553.0358276367188, 511.7721862792969, 521.4656982421875, 463.8973083496094, 546.0366821289062, 599.0281982421875, 574.510009765625, 646.95556640625, 539.6567993164062, 534.114990234375, 533.221435546875, 555.0744018554688, 562.96630859375, 530.0057373046875, 509.9433288574219, 583.0540771484375, 544.92529296875, 552.2523193359375, 553.753173828125, 491.90625, 567.1099853515625, 593.1276245117188, 587.1373291015625, 513.3101806640625, 574.483642578125, 589.6968994140625, 557.581787109375, 599.670166015625, 595.8641967773438, 556.761962890625, 610.4573364257812, 460.87066650390625, 469.80352783203125, 599.99658203125, 517.0023193359375, 661.5834350585938, 572.49951171875, 562.8129272460938, 581.8754272460938, 539.3081665039062, 539.8806762695312, 551.520751953125, 644.58837890625, 536.2698364257812, 499.10687255859375, 542.4521484375, 585.8721923828125, 556.2359619140625, 583.7506103515625, 556.8119506835938, 522.7990112304688, 604.37548828125, 574.116943359375, 559.5623779296875, 556.1771240234375, 523.9917602539062, 505.8202209472656, 559.9534912109375, 562.40185546875, 581.128173828125, 490.4436950683594, 515.09912109375, 514.4056396484375, 635.3531494140625, 610.0089111328125, 557.4946899414062, 566.153564453125, 596.3328857421875, 555.7645263671875, 489.3127746582031, 606.7220458984375, 527.271240234375, 527.4229125976562, 474.23992919921875, 614.6527099609375, 554.8858642578125, 535.1375122070312, 532.389404296875, 564.2131958007812]\n",
            "De werkelijke waarden zijn:\n",
            "[597, 606, 552, 599, 559, 560, 563, 543, 560, 542, 611, 568, 571, 574, 557, 542, 607, 547, 587, 576, 589, 599, 608, 598, 548, 547, 612, 565, 549, 549, 546, 580, 545, 580, 552, 612, 571, 554, 581, 611, 601, 557, 556, 609, 583, 590, 598, 553, 555, 599, 559, 599, 577, 570, 586, 587, 563, 575, 597, 544, 568, 582, 542, 558, 552, 583, 567, 596, 547, 593, 592, 608, 560, 548, 562, 608, 559, 593, 562, 569, 608, 556, 564, 551, 583, 569, 553, 589, 604, 568, 558, 593, 559, 588, 599, 606, 574, 588, 564, 600, 552, 589, 553, 604, 554, 556, 597, 554, 611, 586, 579, 575, 589, 605, 605, 588, 587, 548, 555, 609, 604, 605, 563, 559, 558, 547, 601, 606, 607, 555, 545, 566, 604, 559, 583, 609, 597, 611, 608, 576, 591, 573, 595, 600, 599, 604, 557, 561, 562, 599, 544, 560, 605, 574, 611, 590, 596, 580, 551, 579, 599, 596, 570, 572, 565, 605, 589, 606, 597, 596, 592, 546, 565, 556, 580, 549, 565, 605, 597, 605, 607, 567, 550, 596, 591, 559, 609, 543, 542, 578, 542, 556, 612, 571, 586, 604, 548, 578, 594, 571, 583, 545, 586, 548, 560, 549, 573, 556, 558, 583, 586, 587, 558, 567, 591, 596, 542, 575, 558, 549, 608, 552, 552, 544, 568, 547, 595, 580, 610, 564, 543, 546, 552, 595, 571, 583, 601, 542, 581, 549, 568, 581, 596, 553, 587, 581, 603, 597, 582, 588, 560, 578, 582, 560, 586, 559, 551, 576, 543, 608, 577, 562, 607, 583, 576, 608, 593, 558, 564, 589, 583, 544, 607, 579, 575, 561, 587, 597, 568, 587, 593, 591, 587, 546, 567, 558, 551, 577, 592, 594, 590, 610, 552, 598, 567, 589, 559, 559, 572, 605]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "De errors zijn (afgerond tot gehele waarden):\n",
            "[1, 9, 51, 16, 43, 3, 44, 38, 23, 13, 0, 9, 14, 20, 70, 47, 12, 12, 16, 14, 7, 21, 3, 6, 3, 6, 3, 14, 13, 15, 4, 5, 12, 14, 37, 10, 17, 7, 14, 5, 14, 16, 1, 33, 13, 29, 23, 3, 5, 12, 0, 16, 30, 5, 68, 4, 2, 1, 26, 12, 26, 7, 9, 11, 39, 7, 4, 10, 6, 2, 23, 5, 22, 4, 32, 1, 7, 11, 15, 10, 12, 1, 2, 15, 2, 11, 9, 44, 21, 14, 34, 3, 34, 25, 8, 16, 27, 21, 4, 17, 30, 23, 12, 63, 22, 11, 54, 11, 16, 26, 1, 16, 4, 4, 0, 51, 1, 5, 21, 1, 28, 29, 12, 45, 8, 33, 11, 14, 11, 7, 35, 5, 4, 3, 35, 9, 6, 24, 28, 34, 14, 2, 2, 44, 13, 4, 35, 23, 4, 8, 10, 19, 9, 9, 68, 4, 1, 13, 3, 36, 14, 31, 10, 16, 16, 11, 6, 1, 23, 25, 32, 17, 18, 17, 10, 9, 2, 4, 29, 21, 10, 22, 38, 5, 8, 11, 4, 2, 2, 5, 12, 26, 15, 5, 7, 4, 6, 6, 6, 5, 11, 12, 2, 41, 7, 13, 28, 42, 16, 5, 6, 17, 4, 3, 5, 31, 0, 11, 30, 14, 22, 4, 9, 3, 1, 2, 6, 12, 2, 12, 4, 3, 3, 18, 4, 39, 5, 3, 5, 7, 5, 5, 9, 3, 2, 35, 1, 12, 3, 6, 8, 44, 11, 8, 8, 5, 13, 6, 18, 6, 3, 8, 30, 12, 6, 6, 3, 14, 12, 5]\n",
            "De predicties zijn:\n",
            "[573.6650390625, 566.8619384765625, 537.358642578125, 575.3649291992188, 518.90576171875, 611.8756103515625, 560.5194091796875, 529.24755859375, 580.0863037109375, 591.2781372070312, 604.8411865234375, 574.1882934570312, 564.9400024414062, 585.6586303710938, 536.0340576171875, 552.08447265625, 583.7931518554688, 564.6614990234375, 606.312744140625, 555.798828125, 558.0269775390625, 587.091064453125, 587.1539306640625, 584.3070068359375, 600.8953857421875, 554.7553100585938, 609.0115356445312, 529.3150024414062, 570.241455078125, 562.0625, 558.2909545898438, 553.01171875, 563.3114624023438, 584.816162109375, 539.1890869140625, 607.6954956054688, 576.5916137695312, 547.9240112304688, 598.0919189453125, 590.1530151367188, 559.9550170898438, 569.369140625, 558.3294677734375, 566.556640625, 618.43798828125, 579.587158203125, 534.2981567382812, 585.0610961914062, 587.6683349609375, 579.1529541015625, 600.5919799804688, 569.4195556640625, 574.538818359375, 560.882568359375, 530.31494140625, 613.9700927734375, 586.8160400390625, 604.4962768554688, 560.7202758789062, 554.6390380859375, 581.6431274414062, 554.0399780273438, 600.6766967773438, 553.5152587890625, 563.89599609375, 611.495849609375, 557.9133911132812, 591.2804565429688, 592.5997314453125, 586.322021484375, 577.38818359375, 574.2626953125, 581.483154296875, 580.461181640625, 516.485107421875, 568.52001953125, 564.490478515625, 591.998291015625, 582.091552734375, 612.8378295898438, 573.853759765625, 596.462646484375, 613.5296630859375, 600.4244384765625, 575.6456298828125, 585.3494262695312, 594.818359375, 554.481689453125, 566.7769165039062, 602.9957275390625, 509.044921875, 579.5317993164062, 520.0550537109375, 534.35986328125, 580.924560546875, 605.1791381835938, 561.4745483398438, 569.230224609375, 580.7393798828125, 526.6575927734375, 599.9813232421875, 541.7444458007812, 617.2811889648438, 665.4716796875, 545.822021484375, 580.6962280273438, 649.3473510742188, 619.586669921875, 538.507080078125, 637.1583251953125, 586.6458740234375, 525.7877197265625, 587.8666381835938, 538.3245849609375, 568.3419799804688, 535.4503784179688, 554.3798828125, 602.572021484375, 528.36279296875, 547.6327514648438, 540.0848999023438, 634.2141723632812, 582.3538818359375, 500.4671936035156, 585.268798828125, 549.1842041015625, 617.8655395507812, 606.6839599609375, 589.248779296875, 596.2605590820312, 638.0914306640625, 595.1621704101562, 611.3451538085938, 553.6194458007812, 575.4310913085938, 548.382080078125, 591.6259765625, 608.293701171875, 609.7081298828125, 514.4993896484375, 540.203369140625, 583.5684814453125, 555.6416625976562, 642.8429565429688, 542.2318115234375, 612.576171875, 530.203857421875, 633.9932861328125, 562.560302734375, 565.1431884765625, 537.9605712890625, 528.6837158203125, 546.3706665039062, 580.4492797851562, 676.6190185546875, 593.81396484375, 584.761474609375, 594.3076171875, 610.837890625, 647.8132934570312, 604.6815795898438, 578.80615234375, 593.269287109375, 546.4381103515625, 593.119873046875, 593.156982421875, 572.8514404296875, 561.2749633789062, 541.6713256835938, 554.64990234375, 561.1627197265625, 534.0784912109375, 538.4127197265625, 547.070556640625, 546.804931640625, 586.1029052734375, 547.4973754882812, 604.5316772460938, 580.2725830078125, 589.9485473632812, 584.2695922851562, 568.47314453125, 562.843017578125, 576.325439453125, 559.1513671875, 587.7186279296875, 579.1123657226562, 589.9151000976562, 594.0888671875, 585.167724609375, 566.6394653320312, 584.6103515625, 542.251708984375, 592.2353515625, 555.07080078125, 557.1209106445312, 548.3280639648438, 584.237060546875, 591.2739868164062, 576.2706298828125, 595.77978515625, 560.163330078125, 592.3016357421875, 553.2527465820312, 585.72998046875, 557.9216918945312, 551.712158203125, 567.9708251953125, 588.411865234375, 601.567626953125, 578.1259765625, 592.5660400390625, 590.2644653320312, 558.8395385742188, 595.56103515625, 631.0565185546875, 567.9212036132812, 594.7430419921875, 549.9144897460938, 568.7955322265625, 587.5623168945312, 546.5109252929688, 607.7241821289062, 584.363037109375, 593.0844116210938, 551.2720336914062, 571.5615234375, 549.1083374023438, 569.3602905273438, 572.1056518554688, 568.8887329101562, 594.9343872070312, 593.010009765625, 604.3736572265625, 557.6863403320312, 562.5510864257812, 562.7171630859375, 577.9019775390625, 607.0175170898438, 571.128173828125, 595.0682373046875, 554.981689453125, 570.3844604492188, 559.7547607421875, 603.2515258789062, 564.545654296875, 595.40380859375, 622.4988403320312, 592.94140625, 583.3949584960938, 609.9163208007812, 558.8511352539062, 596.2019653320312, 558.531982421875, 549.5100708007812, 582.9352416992188, 552.6588134765625, 563.8438110351562, 532.252685546875, 595.2954711914062, 566.6234130859375, 589.6976318359375, 529.3343505859375, 566.7874755859375, 591.3555908203125, 612.7845458984375, 545.0692749023438, 589.305419921875, 565.7140502929688, 558.96728515625]\n",
            "De werkelijke waarden zijn:\n",
            "[573, 558, 588, 559, 562, 609, 605, 567, 557, 578, 605, 565, 551, 566, 606, 599, 572, 577, 590, 570, 551, 566, 584, 590, 604, 549, 606, 543, 557, 547, 554, 558, 551, 599, 576, 598, 560, 555, 584, 585, 574, 553, 557, 600, 605, 551, 557, 582, 583, 567, 601, 553, 545, 556, 598, 610, 585, 603, 587, 543, 556, 547, 610, 543, 603, 604, 562, 581, 599, 584, 554, 569, 603, 584, 548, 570, 571, 581, 597, 603, 562, 595, 612, 585, 574, 574, 604, 598, 546, 589, 543, 583, 554, 559, 589, 589, 588, 590, 585, 544, 570, 565, 605, 602, 568, 592, 595, 609, 555, 611, 586, 542, 592, 542, 568, 586, 555, 608, 549, 549, 568, 605, 570, 545, 593, 582, 607, 593, 578, 589, 603, 600, 607, 551, 610, 557, 586, 584, 582, 548, 554, 582, 558, 599, 555, 609, 565, 611, 559, 557, 548, 548, 555, 589, 609, 590, 586, 581, 608, 612, 591, 610, 583, 562, 609, 604, 579, 562, 565, 580, 593, 551, 556, 564, 557, 595, 549, 609, 609, 611, 594, 590, 601, 581, 551, 577, 575, 592, 592, 580, 555, 611, 557, 587, 562, 553, 542, 578, 585, 571, 607, 572, 594, 594, 593, 545, 580, 610, 604, 607, 572, 610, 586, 562, 591, 600, 568, 606, 580, 555, 610, 543, 599, 587, 594, 553, 578, 561, 571, 560, 573, 598, 596, 586, 562, 602, 568, 575, 602, 564, 590, 550, 579, 563, 601, 600, 596, 610, 590, 589, 602, 603, 585, 551, 542, 578, 566, 558, 550, 601, 570, 598, 559, 579, 597, 607, 542, 575, 578, 554]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "De errors zijn (afgerond tot gehele waarden):\n",
            "[7, 59, 39, 12, 29, 2, 53, 54, 15, 91, 319, 34, 708, 182, 15, 11, 214, 21, 70, 52, 41, 7, 13, 37, 79, 35, 20, 43, 30, 130]\n",
            "De predicties zijn:\n",
            "[582.8841552734375, 546.694580078125, 502.5947570800781, 584.5667724609375, 544.03369140625, 562.0654296875, 511.4184265136719, 513.0099487304688, 560.0947265625, 472.68487548828125, 261.76702880859375, 581.7271118164062, -114.94233703613281, 413.63885498046875, 568.0484619140625, 593.2855834960938, 790.39208984375, 595.4098510742188, 526.2791748046875, 601.932861328125, 611.1141967773438, 598.356201171875, 562.927001953125, 571.245361328125, 519.3900146484375, 548.6381225585938, 573.1689453125, 588.4761352539062, 574.2764282226562, 478.3606262207031]\n",
            "De werkelijke waarden zijn:\n",
            "[576, 606, 542, 597, 573, 564, 564, 567, 575, 564, 581, 548, 593, 596, 583, 604, 576, 574, 596, 550, 570, 605, 576, 608, 598, 584, 593, 545, 604, 608]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}